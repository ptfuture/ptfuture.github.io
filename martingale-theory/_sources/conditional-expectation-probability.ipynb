{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c7f1e5-7593-4717-a4a7-53fdd2e9e527",
   "metadata": {},
   "source": [
    "# Conditional Expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced4f074-aac2-45d7-845c-10dea68e2b8d",
   "metadata": {},
   "source": [
    "**Video lecture: [https://youtu.be/RKtSO9hmb1A](https://youtu.be/RKtSO9hmb1A)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525ea1d1-be98-4449-a8cf-d439909e5ac6",
   "metadata": {},
   "source": [
    "Given a probability space $(\\Omega, \\mathcal{F_o}, P)$, a sigma field $\\mathcal{F} \\subset \\mathcal{F_o}$ and a random variable $X \\in \\mathcal{F_o}$ with $E|X| \\lt\\infty$, we define the **condition expectation** of$X$ given $\\mathcal{F}$, $E[X|\\mathcal{F}]$, to be any random variable $Y$ that has:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a87604-b28b-4d98-83e7-1b8314875ed7",
   "metadata": {},
   "source": [
    "1. $Y \\in \\mathcal{F}$\n",
    "2. for all $E \\in \\mathcal{F}$, $\\int_{E}XdP = \\int_{E}YdP$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8678da55-1d27-4573-88f4-7a8d79577879",
   "metadata": {},
   "source": [
    "If there's another random variable also satisfies thise two conditions, they are equal a.s. We denote any random variable that satisfies these two conditions as $E[X|\\mathcal{F}]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de40f6ab-752e-49e0-b9ce-bb1556a07d82",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "If $Y'$ also satisfies 1 and 2 of the conditional expectation definition then $Y$ and $Y'$ are equal almost surely.\n",
    "\n",
    "_Proof:_\n",
    "\n",
    "Let $A = \\{ Y - Y' \\ge \\epsilon \\gt 0\\}$, Since both $Y$ and $Y'$ are $\\mathcal{F}$ measurable, the $Y-Y' \\ge \\epsilon$ is also $\\mathcal{F}$ measurable. By the 2 condition we have:\n",
    "\n",
    "$$\n",
    "0=\\int_{A}X - X dP = \\int_{A}Y - Y' dP \\ge \\epsilon P(A)\n",
    "$$\n",
    "\n",
    "which indicates $P(A) = 0$. Conversely, we could also conclude that $A' = \\{Y' - Y \\ge \\epsilon \\gt 0\\}$ also has measure zero.\n",
    "\n",
    "$\\blacksquare$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bf0a35-0ddd-4dc9-aad8-1958d3e31759",
   "metadata": {},
   "source": [
    "## Radon-Nikodyn(RN) Theorem\n",
    "\n",
    "Let $\\mu$ and $\\nu$ be $\\sigma$-finite measures on $(\\Omega, \\mathcal{F})$. If $\\nu \\ll \\mu$, then there's a $\\mathcal{F}$ measurable function $f$ so that for all $A \\in \\mathcal{F}$:\n",
    "\n",
    "$$\n",
    "\\int_{A}f d\\mu = \\nu(A)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28874286-a9c0-4519-9e14-5e5d34fb453a",
   "metadata": {},
   "source": [
    "$f$ is usually denoted $d\\nu/d\\mu$ and called the **Radon-Nikodym derivative**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b1a95-f590-4b6e-8a27-88156dfd0f4e",
   "metadata": {},
   "source": [
    "- **$\\sigma$-finite**: $\\mu$ is said to be $\\sigma$-finite if there's a increasing sequence of sets $A_n \\in \\mathcal{F}$ that satisfies $A_n \\uparrow \\Omega$ and $\\mu(A_n) \\lt \\infty$ for all $n$.\n",
    "- **$\\nu$ absolute continuous with respect to $\\mu$ ($\\nu \\ll \\mu$)**: If $\\mu(A) = 0$ indicates $\\nu(A) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b876882c-f6e7-4cbc-8e4e-e72c113e86f1",
   "metadata": {},
   "source": [
    "**Existence of probability density function:** Let $\\mu$ be lebesgue measure, then for any probability measure $\\nu$ that is absolute continuous w.r.t $\\mu$ then $\\nu$ has a probability densitiy function which is the Radon-Nikodym derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2a0bd2-5482-4287-82e3-61b4974585a8",
   "metadata": {},
   "source": [
    "**Existence of conditional expectation:** \n",
    "\n",
    "First assume $X \\ge 0$, and $\\mu=P$ and $\\nu(A) = \\int_{A}XdP$ for all $A \\in \\mathcal{F}$. $\\nu$ is a measure (verify disjoin additivity property) and $\\nu\\ll\\mu$ (cause whenevever $P(A) = 0$, we must have $\\nu(A) = 0$). By RN theorem, \n",
    "\n",
    "$$\n",
    "\\int_{A}XdP = \\nu(A) = \\int_{A} \\frac{d\\nu}{d\\mu} dP\n",
    "$$\n",
    "\n",
    "the RN derivative is a version of conditional expectation $E[X|\\mathcal{F}]$. \n",
    "\n",
    "For any $X$, use $X = X^+ - X^-$ we can conclude the existence of conditional expectation for both positive and negative part of $X$ hence the conditional expection of $X$ itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02ab590-3077-4895-9a8e-46c587fd4ed0",
   "metadata": {},
   "source": [
    "## Theorem: $\\sigma$-field generated by partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69de214-97f5-4a69-b1c5-63547922b560",
   "metadata": {},
   "source": [
    "**Video lecture: [https://youtu.be/YdDtsZ5OT2w](https://youtu.be/YdDtsZ5OT2w)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f88806e-423d-47b1-bbcf-2ff059f49c88",
   "metadata": {},
   "source": [
    "Suppose $\\Omega_1,\\Omega_2,...$ is finite or infinite partition of $\\Omega$ into disjoin sets, $P(\\Omega_i) \\gt 0$ for all $i$. Let $\\mathcal{F} = \\sigma(\\Omega_1, \\Omega_2,...)$, then a version of $E[X|\\mathcal{F}]$ is:\n",
    "\n",
    "$$\n",
    "E[X|\\mathcal{F}]_\\omega = \\sum_{i=1}^{\\infty}c_i\\mathbf{1}_{\\omega \\in \\Omega_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ee8792-537d-440b-b941-9a256f49eba2",
   "metadata": {},
   "source": [
    "where $c_i = \\frac{E[X; \\Omega_i]}{P(\\Omega_i)} = \\frac{\\int_{\\Omega_i}XdP}{P(\\Omega_i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12490122-a457-4634-a7de-3054694d120c",
   "metadata": {},
   "source": [
    "_Proof:_\n",
    "\n",
    "Because each $\\mathbf{1}_{\\Omega}$ is $\\mathcal{F}$ measurable, obviously $E[X|\\mathcal{F}]$ is $\\mathcal{F}$ measurable because of it's a summation of simple measurable step functions. To verify the second condition ($\\int_{A}{\\sum_{i=1}^{\\infty}c_i\\mathbf{1}_{\\omega \\in \\Omega_i}}dP = \\int_{A}{X}dP$ for all $A \\in \\mathcal{F}$), note that any event $A \\in \\mathcal{F}$ is a countable union of disjoint $\\Omega_i$. Therefore:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0810810-acdb-4d39-a539-3caed6af4179",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "&\\int_{A}{\\sum_{i=1}^{\\infty}c_i\\mathbf{1}_{\\omega \\in \\Omega_i}}dP \\\\\n",
    "&= \\int{\\mathbf{1}_{\\Omega_{i(1)}}(\\sum_{j=1}^{\\infty}c_j\\mathbf{1}_{\\Omega_j}})dP + \\int{\\mathbf{1}_{\\Omega_{i(2)}}(\\sum_{j=1}^{\\infty}c_j\\mathbf{1}_{\\Omega_j}})dP + ... \\\\\n",
    "&= \\int{c_{i(1)}\\mathbf{1}_{\\Omega_{i(1)}}}dP + \\int{c_{i(2)}\\mathbf{1}_{\\Omega_{i(2)}}}dP ... \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f19b436-20e3-462d-8044-2d1e66d9923a",
   "metadata": {},
   "source": [
    "And we are reduced to prove:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cfa17e-5dae-4f3a-80c4-527cc7804c5a",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\int_{\\Omega_i}c_i dP &= \\int_{\\Omega_i}X dP \\\\\n",
    "c_i \\int_{\\Omega_i} dP &= \\frac{E[X; \\Omega_i]}{P(\\Omega_i)} P(\\Omega_i) = \\int_{\\Omega_i}X dP = E[X;\\Omega_i]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57548892-9c11-49a3-839a-66c1ca47eb4e",
   "metadata": {},
   "source": [
    "## Conditional Probability\n",
    "\n",
    "$$\n",
    "P(A|\\mathcal{F}) = E[\\mathbf{1}_{A}|\\mathcal{F}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2f2776-f588-4fb5-9427-dd804ebd3bfc",
   "metadata": {},
   "source": [
    "If we let $\\mathcal{F} = \\sigma(B, B^c)$, then by the theorem above, the following is a version of $P(A|\\mathcal{F})$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82dc4f8-97f1-4e97-880d-9d2354a245c2",
   "metadata": {},
   "source": [
    "$$\n",
    "P(A|\\mathcal{F}) = \\frac{P(A\\cap B)}{P(B)}\\mathbf{1}_{B} + \\frac{P(A\\cap B^c)}{P(B^c)}\\mathbf{1}_{B^c}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9b895c-bec5-4997-a3c7-f082dab89fc9",
   "metadata": {},
   "source": [
    "Given that experiment $\\omega$ falls in $B$, then \n",
    "\n",
    "$$\n",
    "P(A|\\mathcal{F})_{\\omega} = \\frac{P(A\\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "And this is exactly how the elementary conditional probability comes from, and mathematicians gives this a definition ($P(A|B)$) that you see in elementary probability books:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(A\\cap B)}{P(B)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889c3d32-acbc-4d7f-a824-210ed9486324",
   "metadata": {},
   "source": [
    "## Theorem: Bayes' Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350bf4c1-b580-4f67-92de-809f2806b53c",
   "metadata": {},
   "source": [
    "For any $B \\in \\mathcal{F}$:\n",
    "\n",
    "$$\n",
    "P(B|A) = \\frac{\\int_{B}{P(A|\\mathcal{F})}dP}{\\int_{\\Omega}{P(A|\\mathcal{F})}dP}\n",
    "$$\n",
    "\n",
    "If $\\mathcal{F}$ is a $\\sigma$-field generated by a partition, then this formula reduces to the usual Bayes' formula:\n",
    "\n",
    "$$\n",
    "P(B_i|A) = \\frac{P(A|B_i)P(B_i)}{\\sum_{i=1}^{\\infty}P(A|B_i)P(B_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7f3af4-33b3-47db-860d-330d2ce2a021",
   "metadata": {},
   "source": [
    "_Proof:_\n",
    "\n",
    "$$\n",
    "\\int_{B}P(A|\\mathcal{F})dP = \\int_{B}E(\\mathbf{1}_{A}|\\mathcal{F})dP = P(A\\cap B)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccf888d-23e8-42cd-8ecb-6e06b62ce3e6",
   "metadata": {},
   "source": [
    "$$\n",
    "P(B|A)\\int_{\\Omega}{P(A|\\mathcal{F})}dP = \\frac{P(B\\cap A)}{P(A)}P(A) = P(A\\cap B)\n",
    "$$\n",
    "\n",
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1aee34-9bef-44c2-9bfd-a51380d7a0d2",
   "metadata": {},
   "source": [
    "## Theorem: Conditional expectation with PDF\n",
    "\n",
    "**Video lecture: https://www.youtube.com/watch?v=IJoNHV_Mpbw**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20357f9a-13c7-4d20-949c-abeea2093e35",
   "metadata": {},
   "source": [
    "Suppose $X,Y$ have joint pdf $f(x,y)$, and $\\int{f(x,y)}dx \\not= 0$, and if $E|g(X)| \\lt \\infty$ then $E[g(X)|Y] = h(Y)$ where:\n",
    "\n",
    "$$\n",
    "h(y) = \\frac{\\int{g(x)f(x,y)}dx}{\\int{f(x,y)}dx}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3ccafa-ff5a-46ba-ac2c-30fb75d2ee58",
   "metadata": {},
   "source": [
    "**Lemma**\n",
    "\n",
    "If $f$ is measurable, $f_x(y): y \\mapsto f(x, y)$ is measurable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad528cba-5d1e-43e1-9660-903ff1e1ba06",
   "metadata": {},
   "source": [
    "_Proof of lemma:_\n",
    "\n",
    "First we claim that function $T_x: y \\mapsto (x,y) $ is measurable. This implies $f_x = f \\circ T_x$ is measurable. \n",
    "\n",
    "To prove that $T_x$ is measurable, note that $T_x^{-1}$ is an operator that cross cut the two dimension Borel set on $x$. Define $B_x = \\{y: (x,y) \\in B\\}$ where $B \\in \\mathcal{B}^2$, \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00746c6f-f601-478d-9c92-f387c33b456e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "& T_x^{-1}(B^c) = \\mathbf{R} - T_x^{-1}(B)  \\quad \\text{ implies } \\quad  (B^c)_x = (B_x)^c \\\\\n",
    "& T_x^{-1}(\\cup_i{B_i}) = \\cup_i{T_x^{-1}(B_i)}  \\quad  \\text{ implies }  \\quad   (\\cup B_i)_x = \\cup (B_i)_x\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42c27b9-4bdb-4488-a0d1-54c49f1d22e3",
   "metadata": {},
   "source": [
    "Let $\\mathbb{B} = \\{B: B \\in \\mathcal{B}^2 \\text{ such that } B_x \\in \\mathcal{B} \\}$. \n",
    "\n",
    "- If $B \\in \\mathbb{B}$, then $(B^c)_x = (B_x)^c \\in \\mathcal{B}$ implies $B^c \\in \\mathbb{B}$.\n",
    "- If $B_i \\in \\mathbb{B}$, then $(\\cup_i{B_i})_x = \\cup_i(B_i)_x \\in\\mathcal{B}$ implies $\\cup_i{B_i} \\in \\mathbb{B}$.\n",
    "\n",
    "Therefore $\\mathbb{B}$ is a $\\sigma$-field, and since rectangle borel sets are also included in $\\mathbb{B}$, we have $\\mathcal{B}^2 \\subset \\mathbb{B}$, which concludes $T_x^{-1}(B) = B_x \\in \\mathcal{B}$ for all $B \\in \\mathcal{B}^2$.\n",
    "\n",
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6962798d-06fd-4192-a98e-0b4567c924c6",
   "metadata": {},
   "source": [
    "**Lemma**\n",
    "\n",
    "$y \\mapsto \\int{f(x,y)}dx$ is measurable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b4cc9c-2691-4288-a430-687ae60b0051",
   "metadata": {},
   "source": [
    "_Proof of lemma:_\n",
    "\n",
    "First assume $f \\ge 0$, $f$ measurable implies $y \\mapsto f(x,y)$ is measurable by the lemma above. Define $f_x^{n}(y) = \\frac{[2^nf(x,y)]}{2^n}\\wedge{n}$. $f_x^{n}(y)$ are simple functions that converges upwards to $f(x,y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1327ef-d6dd-47d0-b96d-9d76b65b5610",
   "metadata": {},
   "source": [
    "Measurability of simple function $f_x^{n}(y)$ indicates $y \\mapsto \\int{f_x^{n}(y)}dx$ is measurable. The limit of $y \\mapsto \\int{f_x^{n}(y)}dx$ converges to $y \\mapsto \\int{f(x, y)}dx$ by the monotone convergence theorem. limits of measurable function is measurable, therefore $y \\mapsto \\int{f(x,y)}dx$ is measurable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d7bc8-16f5-4156-b93b-547388688daa",
   "metadata": {},
   "source": [
    "For arbitrary $f$, simply write $f = f^{+} - f^{-}$ to finish our prove.\n",
    "\n",
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4cc337-57c8-4297-b0c6-4a0810bea2ac",
   "metadata": {},
   "source": [
    "_Proof:_\n",
    "\n",
    "First, by the lemma above, $h(y)$ is measurable, so condition 1 of the definition of conditional expectation is met. To prove the second condition, suppose $A \\in \\sigma(Y)$, $A = \\{\\omega: Y(\\omega) \\in B\\}$ for some $B \\in \\mathcal{B}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4376ea3-1d48-448f-a317-30d234ea0426",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\int_{B}\\int{h(y)f(x,y)}dxdy &= \\int_{B}{h(y)\\int{f(x,y)}dx}dy \\\\\n",
    "&= \\int_{B}{\\int{g(x)f(x,y)}dx}dy \\\\\n",
    "&= E[g(X)\\mathbf{1}_{Y(\\omega) \\in B}] = E[g(X);A]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae43886d-20a3-4e87-8deb-d8ed0e5201b1",
   "metadata": {},
   "source": [
    "## Properties of conditional expectation\n",
    "\n",
    "**Video lecture: https://youtu.be/Uk-HBqB7HnA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b539b3e3-5932-4048-bf4c-c63cc0a31172",
   "metadata": {},
   "source": [
    "\n",
    "1. Linearity: $E[aX+bY|\\mathcal{F}] = aE[X|\\mathcal{F}] + bE[Y|\\mathcal{F}]$\n",
    "2. Monotonicty: $X \\le Y$ implies $E[X|\\mathcal{F}] \\le E[Y|\\mathcal{F}]$\n",
    "3. Chebyshev's inequality: Suppose $\\phi: \\mathbf{R} \\mapsto \\mathbf{R}$, and $\\phi \\ge 0$, Let $A\\in\\mathcal{B}$ and $i_a = \\inf\\{\\phi(x): x \\in A\\}$, then $i_AP(X\\in A | \\mathcal{F}) \\le E[\\phi(X)|\\mathcal{F}]$\n",
    "4. Markov inequality: For any $a \\in \\mathbf{R}$, $a^2P(|X| \\ge a | \\mathcal{F}) \\le E[X^2|\\mathcal{F}]$\n",
    "5. Monotone convergence: If $X_n \\ge 0$ and $X_n\\uparrow X$ with $E|X| \\lt \\infty$, then $E[X_n|\\mathcal{F}]$\n",
    "6. Jenson inequality: If $\\phi$ is convex and $E|X| \\lt \\infty$ and $E|\\phi(X)| \\lt \\infty$ then $\\phi(E[X|\\mathcal{F}]) \\le E[\\phi(X)|\\mathcal{F}]$\n",
    "7. Cauchy-Schwarz inequality: $E[XY|\\mathcal{F}]^2 \\le E[X^2|\\mathcal{F}]E[Y^2|\\mathcal{F}]$\n",
    "8. $E[E[X|\\mathcal{F}]] = E[X]$\n",
    "9. Given $\\mathcal{F_1} \\subset \\mathcal{F_2}$ then: $E[E[X|\\mathcal{F_1}]|\\mathcal{F_2}] = E[X|\\mathcal{F_1}]$\n",
    "10. Given $\\mathcal{F_1} \\subset \\mathcal{F_2}$ then: $E[E[X|\\mathcal{F_2}]|\\mathcal{F_1}] = E[X|\\mathcal{F_1}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f24a0a-116a-45f3-a43f-b65b2cd8c342",
   "metadata": {},
   "source": [
    "## Theorem\n",
    "\n",
    "If $X\\in \\mathcal{F}$, $E|Y| \\lt \\infty$ and $E|XY| \\lt \\infty$ then $E[XY|\\mathcal{F}] = XE[Y|\\mathcal{F}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29cbc01-1cd0-493d-906a-397fe1dd690d",
   "metadata": {},
   "source": [
    "_Proof:_\n",
    "\n",
    "$XE[Y|\\mathcal{F}]$ is obviously $\\mathcal{F}$ measurable. Only need to check the second condition. We first check when $X = \\mathbf{1}_A$ for any $A\\in\\mathcal{F}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622c1a85-9641-4100-be93-76b047b2139f",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\int_{E} E[\\mathbf{1}_{A}Y|\\mathcal{F}]dP &= \\int_{E} \\mathbf{1}_{A}YdP \\\\\n",
    "&= \\int_{E \\cap A}YdP \\\\\n",
    "&= \\int_{E \\cap A}E[Y|\\mathcal{F}]dP \\\\\n",
    "&= \\int_{E}\\mathbf{1}_{A}E[Y|\\mathcal{F}]dP\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2925b7f-34f3-420f-a627-2a55e5777284",
   "metadata": {},
   "source": [
    "Therefore its also true when $X$ is simple positive random variable. Then further to positive $X$ where $X$ is the limit of increasing positive simple random variables $X_n \\uparrow X$. Last, for any $X = X^+-X^-$.\n",
    "\n",
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2e89f9-0049-4024-984a-5e01260d78a7",
   "metadata": {},
   "source": [
    "## Theorem: Mean square error and Orthogonality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872647f4-a44b-4776-9bee-3b455ebb7432",
   "metadata": {},
   "source": [
    "Suppose $X \\in L^2(\\mathcal{F_o})$, where $L^2(\\mathcal{F_o}) = \\{Y \\in \\mathcal{F_o}: E[Y^2] \\lt \\infty\\}$.  $L^2(\\mathcal{F}) \\subset L^2(\\mathcal{F_o})$.\n",
    "\n",
    "$E[X|\\mathcal{F}]$ is the random variable $Y \\in \\mathcal{F}$ that minimized $E[(X-Y)^2]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618eadb2-4e27-4925-a790-083f0ab05ab5",
   "metadata": {},
   "source": [
    "_Proof:_\n",
    "\n",
    "If $Y \\in L^2(\\mathcal{F})$, let $Z = E[X|\\mathcal{F}] - Y$, then:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[(X-Y)^2] &= E[(X-E[X|\\mathcal{F}] + Z)^2] \\\\\n",
    "&= E[(X-E[X|\\mathcal{F}])^2] + 2E[Z(X-E[X|\\mathcal{F}])] + E[Z^2]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca762c3-fc1c-4cf3-8347-ff887b5ca5c6",
   "metadata": {},
   "source": [
    "The middle term is zero:\n",
    "\n",
    "$$\n",
    "E[Z(X-E[X|\\mathcal{F}])] = E[ZX] - E[E[ZX|\\mathcal{F}]] = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775ca9d1-e4b9-4696-9021-c30ca1d5aced",
   "metadata": {},
   "source": [
    "$E[(X-Y)^2]$ is minimized when $Z = 0$.\n",
    "\n",
    "$\\blacksquare$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
