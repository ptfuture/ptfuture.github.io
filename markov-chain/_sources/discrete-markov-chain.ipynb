{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Time Markov Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Video lecture: https://youtu.be/8WHj9rFvYYU**\n",
    "\n",
    "Consider a stochastic process $\\{ X_n, n = 0, 1, 2,...\\}$ that takes finite or countable number of possible values. \n",
    "\n",
    "If $X_n=i$ we say the process is in state i at time n. We suppose that when the process is in state i, the probability that the process will enter state $j$ is $\\boldsymbol{P}_{ij}$.\n",
    "\n",
    "Define the discrete time markov chain as: \n",
    "\n",
    "$$\n",
    "P(X_{n+1} = j | X_n = i, X_{n-1} = i_{n-1}, ..., X_1 = i_1, X_0 = i_0) = P(X_{n+1} = j | X_n = i) = \\boldsymbol{P}_{ij}\n",
    "$$\n",
    "\n",
    "Markov chain is a **Memoryless Process**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapman-Komogorov Equantions\n",
    "\n",
    "Chapman-Komogorov Equantions provides a method for computing the n-step transition probabilities $\\boldsymbol{P}_{ij}^n$\n",
    "\n",
    "Define $P_{ij}^n$ be the probability that a process in state $i$ will be in state $j$ after n transitions.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{P}_{ij}^n = P(X_{n+k} = j | X_{k} = i), \\qquad n \\ge 0\n",
    "$$\n",
    "\n",
    "**Chapman-Komogorov Equantions**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\displaystyle \\boldsymbol{P}_{ij}^{n+m} &= P(X_{n+m} = j|X_0=i) \\\\\n",
    "      &= \\sum_{k=0}^\\infty{P(X_{n+m}=j,X_n=k|X_0=i)} \\\\\n",
    "      &= \\sum_{k=0}^\\infty{P(X_{n+m}=j|X_n=k,X_0=i)P(X_n=k|X_0=i)} \\\\\n",
    "      &= \\sum_{k=0}^{\\infty} \\boldsymbol{P}_{ik}^n \\boldsymbol{P}_{kj}^m\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite State\n",
    "\n",
    "Finate state markov chain: $X_n$ could only be one of the finite number of states.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\boldsymbol{P} = \\begin{bmatrix}\n",
    "    p_{11} & p_{12} & p_{13} & p_{14} \\\\\n",
    "    p_{21} & p_{22} & p_{23} & p_{24} \\\\\n",
    "    ... ... \\\\\n",
    "    p_{41} & p_{42} & p_{43} & p_{44} \\\\\n",
    "  \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\displaystyle \\sum_{k=1}^n{p_{ik}} = 1, \\forall i \\in (1,...,n)$\n",
    "\n",
    "$\\boldsymbol{P^{n+m}} = \\boldsymbol{P}^n\\boldsymbol{P}^m$, therefore, $\\boldsymbol{P}_{ij}^{n+m} = (\\boldsymbol{P}^n\\boldsymbol{P}^m)_{ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of States\n",
    "\n",
    "**Video lecture: https://youtu.be/kZEosBNtXwQ**\n",
    "\n",
    "\n",
    "- State $j$ is **accessible** from state $i$ if $\\boldsymbol{P}_{ij}^n \\gt 0$ for some $n \\ge 0$. We write it as $i \\rightarrow j$.\n",
    "- If state $i$ and state $j$ are said to **communicate** if $\\boldsymbol{P}_{ij}^n \\gt 0$ for some $n \\ge 0$ and $\\boldsymbol{P}_{ji}^m \\gt 0$ for some $m \\ge 0$. We write it as $i \\leftrightarrow j$.\n",
    "- Any state communicates with itself, because $\\boldsymbol{P}_{ii}^0 = P(X_0=i|X_0=i) = 1$.\n",
    "- If $i \\to j$ and $j \\to k$, then $i \\leftrightarrow k$, because $\\boldsymbol{P}_{ik}^{n+m} = \\sum_{d=0}^{\\infty} \\boldsymbol{P}_{id}^n \\boldsymbol{P}_{dk}^m \\ge \\boldsymbol{P}_{ij}^n\\boldsymbol{P}_{jk}^m \\gt 0$ for some $n \\ge 0$ amd $m \\ge0$.\n",
    "- Communicattion divides the state space into separate **classes**.\n",
    "- The Markov chain is said to be **irreducible** if all states belongs to one class. That is all states communicates with each other.\n",
    "- State $j$ called **absorbing state** if once entered can never leave.\n",
    "  $$\n",
    "  \\begin{bmatrix}\n",
    "    1 & 0 & 0 \\\\\n",
    "    1/3 & 1/3 & 1/3 \\\\\n",
    "    1/4 & 1/4 & 1/2\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "\n",
    "### Recurrent state/Transient state\n",
    "\n",
    "Define an identity random variable:\n",
    "\n",
    "$$\n",
    "\\mathbf{1}_{(X_n = j)} = \\begin{cases}\n",
    "  1, & \\text{if } X_n = j \\\\\n",
    "  0, & \\text{if } X_n \\ne j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $n \\ge 0$. \n",
    "\n",
    "Define $P_i$ as the probability measure given intial state is $i$. That is\n",
    "\n",
    "$$\n",
    "P_i(X_n = j) = P(X_n = j | X_0 = i)\n",
    "$$\n",
    "\n",
    "Define $T_j^m$ be the number of transition needed for the process to reenter state $j$ exactly $m$ times:\n",
    "\n",
    "$$\n",
    "T_{j}^m = \\inf\\{n \\ge 1; \\mathbf{1}_{(X_1=j)} + \\mathbf{1}_{(X_2=j)} + \\mathbf{1}_{(X_3=j)} +,...+ \\mathbf{1}_{(X_n=j)} = m\\}\n",
    "$$\n",
    "\n",
    "\n",
    "Then $\\rho_{ij} = P_i(T_j  \\lt \\infty)$ is the probability that the process firstly visits state $j$ in finitely many transitions.\n",
    "\n",
    "\n",
    "By the bayes chain rule and markov memoryless property, we can derive the probability that the process visits state $j$ for $m$ timesÂ in finitely many transitions. Which is the probability of transition from state $i$ to state $j$ for the first time multiplies the probability of re-visiting state $j$ for $m-1$ times.\n",
    "$$\n",
    "P_i(T_j^m \\lt \\infty) = \\rho_{ij}\\rho_{jj}^{m-1}  \\tag{1}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We say $j$ is **recurrent** if $\\rho_{jj} = 1$. If state $j$ is recurrent then, starting in state $j$, the process will reenter state $j$ infinitely often. \n",
    "\n",
    "We say $j$ is **transient** if $\\rho_{jj} \\lt 1$. That is there's chance $1-\\rho_{jj}$ that the process will never return to state $j$. \n",
    "\n",
    "\n",
    "Define $N_j$ as the number of times the process reenters state $j$. $\\displaystyle N_j=\\sum_{n=1}^{\\infty}{\\mathbf{1}_{(X_n=j)}}$. \n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\displaystyle E_i[N_j] &= \\sum_{n=1}^{\\infty} nP_i(N_j = n) = \\sum_{n=1}^{\\infty} \\sum_{m=1}^n P_i(N_j = n) = \\sum_{m=1}^{\\infty} \\sum_{n=m}^{\\infty} P_i(N_j = n) \\\\\n",
    "    &= \\sum_{m=1}^{\\infty} P_i(N_j \\ge m) = \\sum_{m=1}^{\\infty} (1 - P_i(N_j \\le m-1)) \\\\\n",
    "    &= \\sum_{m=1}^{\\infty} (1- P_i(T_j^m = \\infty)) = \\sum_{m=1}^{\\infty} P_i(T_j^m \\lt \\infty) \\\\\n",
    "    & = \\sum_{m=1}^{\\infty} \\rho_{ij}\\rho_{jj}^{m-1} = \\frac{\\rho_{ij}}{1-\\rho_{jj}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Alternative approach:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  E_i[N_j] &= E_i[\\sum_{n=1}^{\\infty}{\\mathbf{1}_{(X_n=j)}}] = \\sum_{n=1}^{\\infty} E_i[\\mathbf{1}_{(X_n=j)}] = \\sum_{n=1}^{\\infty} [1 \\times P_i(\\mathbf{1}_{n} = 1) + 0 \\times P_i(\\mathbf{1}_{n} = 0)] \\\\\n",
    "    &= \\sum_{n=1}^{\\infty} P(X_n=j | X_0=i) \\\\\n",
    "    & = \\sum_{n=1}^{\\infty} \\boldsymbol{P}_{ij}^n\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\displaystyle E_i[N_j] = \\frac{\\rho_{ij}}{1-\\rho_{jj}} = \\sum_{n=1}^{\\infty} \\boldsymbol{P}_{ij}^n \\qquad \\tag{2}\n",
    "$$ \n",
    "\n",
    "If we let $i$ be the same as $j$, then: \n",
    "\n",
    "$$\n",
    "\\displaystyle E_j[N_j] = \\frac{\\rho_{jj}}{1-\\rho_{jj}} = \\sum_{n=1}^{\\infty} \\boldsymbol{P}_{jj}^n \\quad \\tag{3}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "If a process is 100% sure to return back to its origin state, then the average of times of returns is infinite. If a process is not 100% sure to return to its origin, then the average of times of returns is finite.\n",
    "\n",
    "### Theorem\n",
    "\n",
    "\n",
    "\n",
    "State $j$ is recurrent iff $E_j[N_j] = \\sum_{n=1}^{\\infty} \\boldsymbol{P}_{jj}^n = \\infty$ \n",
    "State $j$ is transient iff $E_j[N_j] = \\sum_{n=1}^{\\infty} \\boldsymbol{P}_{jj}^n \\lt \\infty$.\n",
    "\n",
    "\n",
    "\n",
    "Proof: \n",
    "**(First approach)**\n",
    "Theorem already proved by equation (2).\n",
    "\n",
    "**(Second approach)**\n",
    "$N_j$ is actually a geometric random variable with $\\rho_{jj} = P_j(T_j \\lt \\infty)$ the probability that the process ever reenters state $j$. For exactly n times the process reenters state $i$, the probability equals to $\\rho_{jj}^{n}(1-\\rho_{jj})$. So the expected value of a geometric random variable is $E[N_i] = \\frac{\\rho_{jj}}{1-\\rho_{jj}}$. When $j$ is reccurent we have $E_j[N_j] = \\infty$ and when $j$ is transient $E_j[N_j] \\lt \\infty$\n",
    "\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "\n",
    "### Theorem\n",
    "\n",
    "If state $i$ is recurrent and $i\\leftrightarrow{j}$, then state $j$ is recurrent.\n",
    "If state $i$ is transient and $i\\leftrightarrow{j}$, then state $j$ is transient.\n",
    "\n",
    "\n",
    "\n",
    "Proof:\n",
    "\n",
    "$P_{ii}^{k+n+s} \\ge P_{ij}^k P_{jj}^n P_{ji}^s$, Therefore, $i$ is transient indicates $\\infty \\gt \\sum{P_{ii}^{k+n+s}} \\ge P_{ij}^k P_{ji}^s \\sum{P_{jj}^n}$. Same logic applies for recurrent.\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "### Theorem (Borel-Cantelli's result)\n",
    "\n",
    "$P(X_n = j \\quad \\text{i.o.}) = 1$ iff $\\sum_{n=1}^{\\infty} \\boldsymbol{P}_{jj}^n = \\infty$\n",
    "$P(X_n = j \\quad \\text{i.o.}) = 0$ iff $\\sum_{n=1}^{\\infty} \\boldsymbol{P}_{jj}^n \\lt \\infty$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationary Distribution\n",
    "\n",
    "**Video lecture: https://youtu.be/_F4jk_9tyAA**\n",
    "\n",
    "\n",
    "### Definition\n",
    "\n",
    "<div class=\"uk-box-shadow-small uk-padding-small uk-background-muted\">\n",
    "\n",
    "A discrete time markov chain $\\{ X_n; n \\ge 0\\}$ is said **stationary** or **stationary in time** if for any time points $n_1, n_2, ...n_k$ and any $t \\ge 0$, The join distribution of $(X_{n_1}, X_{n_2},...,X_{n_k})$ is the same as $(X_{n_{1+t}}, X_{n_{2+t}},...,X_{n_{k+t}})$\n",
    "\n",
    "</div>\n",
    "\n",
    "### Definition\n",
    "\n",
    "<div class=\"uk-box-shadow-small uk-padding-small uk-background-muted\">\n",
    "\n",
    "A probability distribution $\\pi_i, i\\in S$ is called **stationary distribution** if it satisfies:\n",
    "\n",
    "$$\n",
    "\\displaystyle \\sum_{i \\in S}{\\pi_i}{p_{ij}}=\\pi_j, \\qquad \\sum{\\pi_i} = 1\n",
    "$$\n",
    "\n",
    "let $\\boldsymbol{\\pi} = [\\pi_1, \\pi_2, \\pi_3, ...]$, and $\\boldsymbol{P}$ be the transitino matrix:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\pi}\\boldsymbol{P} = \\boldsymbol{\\pi}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\pi}\\boldsymbol{P}^n = \\boldsymbol{\\pi}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\displaystyle \\sum_{i \\in S}{\\pi_i}{p_{ij}^n}=\\pi_j, \\qquad \\sum{\\pi_i} = 1\n",
    "$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Existence of Stationary Distribution\n",
    "\n",
    "Under what conditions does a stationary distribution exist?\n",
    "\n",
    "Let $i$ be a recurrent state, and $T_i=\\inf\\{n \\ge 1: X_n=i\\}$, define $\\mu_i(y)$ as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\mu_i(j) &= E_i\\bigg(\\sum_{n=0}^{T_i-1}\\boldsymbol{1}_{\\{X_n=j\\}}\\bigg) = E\\bigg(E_i \\bigg( \\sum_{n=0}^{T_i-1}\\boldsymbol{1}_{\\{X_n=j\\}} | T_i \\bigg) \\bigg) \\\\\n",
    "  &= \\sum_{m=1}^{\\infty} \\sum_{n=0}^{m-1} P_i(X_n=j, T_i=m) = \\sum_{n=0}^{\\infty} \\sum_{m=n+1}^{\\infty}  P_i(X_n=j, T_i=m) \\\\\n",
    "  &= \\sum_{n=0}^{\\infty} P_i(X_n=j, T_i \\gt n)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\mu_i(i) = 1$ because: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\mu_i(i) &= \\sum_{n=0}^{\\infty} P_i(X_n=i, T_i \\gt n) \\\\\n",
    "  &= P_i(X_0=i, T_i \\gt 0) + P_i(X_1=i, T_i \\gt 1) + \\text{ ...} \\\\\n",
    "  &= P_i(X_0=i, T_i \\gt 0) = 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Summing over all state $j$ that happens in one return period is the exepcted time of first return: \n",
    "\n",
    "$$\n",
    "\\sum_{j}\\mu_i(j) = \\sum_{j} \\sum_{n=0}^{\\infty} P_i(X_n=j, T_i \\gt n) = E_i(T_i)\n",
    "$$\n",
    "\n",
    "Given that $E_iT_i \\lt \\infty$, $\\frac{\\mu_i(j)}{E_i(T_i)}$ sounds like a potential candidate of a stationary distribution if it ever exist. We'll find out.\n",
    "\n",
    "### Definition\n",
    "\n",
    "A recurrent state $i$ is called positive recurrent if it's recurrent and $E_iT_i \\lt \\infty$.\n",
    "A recurrent state $i$ is called null recurrent if it's recurrent and $E_iT_i = \\infty$.\n",
    "\n",
    "### Theorem\n",
    "\n",
    "If $i$ is recurrent, then $\\boldsymbol{\\mu_i}$ satifies $\\boldsymbol{\\mu_iP}=\\boldsymbol{\\mu_i}$, i.e. $\\sum_{j}\\mu_i(j)p_{jk} = \\mu_i(k)$\n",
    "\n",
    "Proof:\n",
    "\n",
    "If $k \\ne i$, then \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\sum_{j}\\mu_i(j)p_{jk} &= \\sum_{j} \\sum_{n=0}^{\\infty} P_i(X_n=j, T_i \\gt n) p_{jk} \\\\\n",
    "  &= \\sum_{n=0}^{\\infty} \\sum_{j} P_i(X_n=j, T_i \\gt n, X_{n+1}=k) \\\\\n",
    "  &= \\sum_{n=0}^{\\infty} P_i(X_{n+1} = k, T_i \\gt n) \\\\\n",
    "  &= \\sum_{n=0}^{\\infty} P_i(X_{n} = k, T_i \\gt n) \\qquad \\text{by } P_i(X_{0} = k, T_i \\gt n) = 0 \\\\\n",
    "  &= \\mu_i(k)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If $k = i$, then\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\sum_{j}\\mu_i(j)p_{ji} &= \\sum_{j} \\sum_{n=0}^{\\infty} P_i(X_n=j, T_i \\gt n) p_{ji} \\\\\n",
    "  &= \\sum_{n=0}^{\\infty} \\sum_{j} P_i(X_n=j, T_i \\gt n, X_{n+1}=i) \\\\\n",
    "  &= \\sum_{n=0}^{\\infty} P_i(X_{n+1} = i, T_i \\gt n) \\\\\n",
    "  &= \\sum_{n=0}^{\\infty} P_i(T_i = n+1) = P_i(T_i \\lt \\infty) = 1 \\\\\n",
    "  &= \\mu_i(i)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "\n",
    "Therefore, if $i$ is positive recurrent, then $\\pi(j) = \\frac{\\mu_i(j)}{E_iT_i}$ is a solution that satifies definition of stationary distribtion, because, by the theorem above, it satisfies $\\boldsymbol{\\mu_iP} = \\boldsymbol{\\mu_i}$ (Let $\\pi = \\mu_i$), And $\\sum{\\pi(j)} =\\sum_j\\frac{\\mu_i(j)}{E_iT_i} = \\frac{E_iT_i}{E_iT_i} = 1$.\n",
    "\n",
    "But that doesn't gaurantee the uniqueness of the solution. \n",
    "Note that if $j$ is also positive recurrent, is $\\frac{\\mu_j(j)}{E_jT_j} = \\frac{1}{E_jT_j}$ is another candidate or are they equal?\n",
    "\n",
    "It sounds weird for a Markov chain in real life to have two or more different stationary distributions. Although possible, think of a markov chain that have two different recurrent classes, depending on which class the initial state belongs to, it may ends in a different stationary distribution.\n",
    "\n",
    "Our interests here is when does the stationary distribution unique?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniqueness of Stationary Distribution\n",
    "\n",
    "**Video lecture: https://youtu.be/lZrzjxQ3YF8**\n",
    "\n",
    "### Theorem\n",
    "\n",
    "\n",
    "If Markov chain $\\boldsymbol{P}$ is irreducible and $i$ recurrent, then the stationary measure $\\mu_i$ is unique up to constant multiples.\n",
    "\n",
    "\n",
    "_Proof:_\n",
    "\n",
    "Suppose $\\nu$ is another stationary measure. $\\nu(k) = \\sum_{j} \\nu(j)p_{jk}$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\nu(k) &= \\nu(i)p_{ik} + \\sum_{j \\ne i} \\nu(j)p_{jk} \\\\\n",
    "  &= \\nu(i)p_{ik} + \\sum_{j \\ne i} (\\nu(i)p_{ij} + \\sum_{h \\ne i} \\nu(h)p_{hj})p_{jk} \\\\\n",
    "  &= \\nu(i)p_{ik} + \\sum_{j \\ne i} \\nu(i)p_{ij}p_{jk} + \\sum_{j \\ne i} \\sum_{h \\ne i} \\nu(h)p_{hj}p_{jk} \\\\\n",
    "  &= \\nu(i) \\sum_{m=1}^{n} P_i(X_g \\ne i, 1 \\le g \\lt m, X_m = k) +  ...\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let $n \\rightarrow \\infty$, therefore, $\\nu(k) \\ge \\nu(i) \\mu_i(k)$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\nu(i) &= \\sum_{k} \\nu(k)p_{ki} \\ge \\nu(i) \\sum_{k} \\mu_i(k) p_{ki} = \\nu(i) \\mu_i(i) = \\nu(i)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "Therefore, it must be $\\sum_{k} \\nu(k)p_{ki} = \\nu(i) \\sum_{k} \\mu_i(k) p_{ki}$. \n",
    "However $\\nu(k) \\ge \\nu(i) \\mu_i(k)$ so it must be $\\nu(k) = \\nu(i) \\mu_i(k)$. \n",
    "Since $\\boldsymbol{P}$ is irreducible, so this is true for all state $k \\in S$.\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "\n",
    "### Corollary\n",
    "\n",
    "If $\\boldsymbol{P}$ is irreducible and recurrent, then $\\mu_i(j)\\mu_j(k)=\\mu_i(k)$, $\\frac{\\mu_i(j)}{E_i(T_i)} = \\frac{1}{E_j(T_j)}$. \n",
    "\n",
    "_Proof:_\n",
    "\n",
    "Since $\\boldsymbol{P}$ is irreducible, all states are recurrent. Theorem above indicates $\\mu_i(j)\\mu_j(k)=\\mu_i(k)$, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\sum_{k}\\mu_i(j)\\mu_j(k) &= \\sum_{k}\\mu_i(k) \\\\\n",
    "  \\mu_i(j)E_j(T_j) &= E_i(T_i) \\\\\n",
    "  \\frac{\\mu_i(j)}{E_i(T_i)} &= \\frac{1}{E_j(T_j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "\n",
    "### Theorem\n",
    "\n",
    "If there's a stationary distribution then all states $j$ that have $\\pi_j \\gt 0$ are recurrent.\n",
    "\n",
    "_Proof:_\n",
    "\n",
    "Since $\\boldsymbol{\\pi}\\boldsymbol{P}^n=\\boldsymbol{\\pi}$ and $\\pi_j \\gt 0$:\n",
    "\n",
    "$$\n",
    "\\displaystyle \\sum_{i}{\\pi_i}\\sum_{n=1}^{\\infty}{p_{ij}^n} = \\sum_{n=1}^{\\infty} \\sum_{i}{\\pi_i}{p_{ij}^n} = \\sum_{n=1}^{\\infty} \\pi_j = \\infty\n",
    "$$\n",
    "\n",
    "But: \n",
    "\n",
    "$$\n",
    "\\infty = \\sum_{i}{\\pi_i}\\sum_{n=1}^{\\infty}{p_{ij}^n} = \\sum_{i} \\pi_i \\frac{\\rho_{ij}}{1-\\rho_{jj}} \\le \\sum_{i} \\pi_i \\frac{1}{1-\\rho_{jj}} \\le \\frac{1}{1-\\rho_{jj}}\n",
    "$$\n",
    "\n",
    "Therefore, $\\rho_{jj} = 1$\n",
    "\n",
    "\n",
    "### Theorem\n",
    "\n",
    "If $\\boldsymbol{P}$ is irreducible, following are equivalent:\n",
    "\n",
    "1. There exist a stationary distribution\n",
    "2. The stationary distribution is unique\n",
    "3. Some $x$ is positive recurrent\n",
    "4. All states are positive recurrent\n",
    "\n",
    "\n",
    "Existence of stationary distribution implies $\\pi(i) \\gt 0$ for some $i$. therefore $i$ is recurrent, irreducibility tells us all states are recurrent, therefore, there must be some state $j$ such that $0 \\lt \\pi(j) \\lt \\infty$, to satisfy the condition $\\sum\\pi(s) = 1$. therefore, $j$ is positive recurrent. But $\\frac{\\mu_i(j)}{E_iT_i} = \\frac{1}{E_jT_j} = \\pi(j)$, $0 \\lt \\frac{\\mu_i(j)}{E_iT_i} \\lt \\infty$ implies $E_iT_i \\lt \\infty$, for all states $i$. Therefore all states are positive recurrent. \"Unique upto constant multiples of stationary measure $\\mu_i$\" implies uniqueness of stationary distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asymptotic Behavior\n",
    "\n",
    "**Video lecture: https://youtu.be/TDyHLfbccq4**\n",
    "\n",
    "If $j$ is transient $\\sum{p^n(i,j)} \\lt \\infty$ then $p^n(i,j) \\rightarrow 0$ as $n \\rightarrow \\infty$. What if $j$ is recurrent, will $p^n(x,y)$ converge?\n",
    "\n",
    "Let $N_n(j)$ be the number of visits to $j$ by time n.\n",
    "\n",
    "$$\n",
    "N_n(j) = \\sum_{m=1}^{n}{\\mathbf{1}_{\\{X_m=j\\}}}\n",
    "$$\n",
    "\n",
    "### Theorem\n",
    "\n",
    "Let $j$ be a recurrent state. For any $i \\in S$, as $n \\rightarrow \\infty$\n",
    "\n",
    "$$\n",
    "\\frac{N_n(j)}{n} \\rightarrow \\frac{1}{E_j(T_j)} \\quad P_i\\text{-a.s.}\n",
    "$$\n",
    "\n",
    "### Proposition\n",
    "\n",
    "Let $\\{X_m, n \\ge 1\\}$ be an irreducible Markov chain with stationary probability $\\pi_j, j\\ge 1$, let $g$ be a bounded function on the state space, Then as $n \\rightarrow \\infty$:\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{m=1}^n{g(X_m)}}{n} \\rightarrow \\sum_{j=1}^{\\infty} g(j)\\pi_j = \\sum_{j=1}^{\\infty} \\frac{g(j)}{E_j(T_j)} \\quad \\text{a.s.}\n",
    "$$\n",
    "\n",
    "Proof:\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{m=1}^n{g(X_m)}}{n} = \\frac{\\sum_{j=1}^{\\infty}{ \\sum_{m=1}^n{ g(X_m) \\mathbf{1}_{\\{X_m=j\\}} } }}{n} \\xrightarrow{a.s.} \\sum_{j=1}^{\\infty}\\frac{g(j)}{E_j(T_j)}\n",
    "$$\n",
    "\n",
    "\n",
    "### Definition\n",
    "\n",
    "$d_i = gcd(\\{ n \\ge 1; p_{ii}^n \\gt 0\\})$ is called the period of $i$.\n",
    "\n",
    "### Lemma\n",
    "\n",
    "Let $i$ be recurrent, if $\\rho_{ij} \\gt 0$, then $d_i=d_j$.\n",
    "\n",
    "### Lemma\n",
    "\n",
    "If $d_i = 1$ then there exist a $m_0$ such that $p^m(i,i) \\gt 0$ for all $m \\gt m_0$.\n",
    "\n",
    "\n",
    "### Theorem\n",
    "\n",
    "Let $p$ be an irreducible aperiodic Markov chain with stationary distribution $\\pi$. then as $n \\rightarrow \\infty$, $p^n(i,j) \\rightarrow \\pi_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Reversible Markov Chain\n",
    "\n",
    "**Video lecture: https://youtu.be/AIweEzM-Mgo**\n",
    "\n",
    "Consider a stationary ergodic Markov chain with transition probability $p(i,j)$ and stationary distribution $\\pi(i)$, if we reverse the process, we'll get a reversed Makrov chain with transition probability $q(j,i)$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q(j,i) &= P(X_m=i|X_{m+1} = j) \\\\\n",
    "&= \\frac{P(X_m=i, X_{m+1}=j)}{P(X_{m+1} = j)} \\\\\n",
    "&= \\frac{P(X_{m+1}=j|X_m=i)P(X_m=i)}{P(X_{m+1}=j)} \\\\\n",
    "&= \\frac{\\pi(i)p(i,j)}{\\pi(j)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pi(i)p(i,j)=\\pi(j)q(j,i)\n",
    "$$\n",
    "When $q(j,i)=p(i,j)$, it's called time-reversible Markov chain.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\pi(i)p(i,j)=\\pi(j)p(j,i) \\tag{4}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The rate at which it goes from $i$ to $j$ equals to the rate at which it goes from $j$ to $i$. \n",
    "\n",
    "If $p$ is time-reversible, then we could find the stationary distribution using (4), because it's a necessary condition for time-reversible markov chain. However, you may not find a solution in many cases, because it's a sufficient condition.\n",
    "\n",
    "For example, if $p$ is time-reversible then the following is true:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\pi(i)p(i,j)=\\pi(j)p(j,i) \\\\\n",
    "\\pi(k)p(k,j)=\\pi(j)p(j,k) \\\\\n",
    "\\pi(i)p(i,k)=\\pi(k)p(k,i) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The first two equation tells us:\n",
    "\n",
    "$$\n",
    "\\frac{\\pi(i)}{\\pi(k)} = \\frac{p(k,j)p(j,i)}{p(i,j)p(j,k)}\n",
    "$$\n",
    "The last equation tells us:\n",
    "\n",
    "$$\n",
    "\\frac{\\pi(i)}{\\pi(k)}=\\frac{p(k,i)}{p(i,k)}\n",
    "$$\n",
    "Combining this two we get:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{p(k,j)p(j,i)}{p(i,j)p(j,k)} &= \\frac{p(k,i)}{p(i,k)} \\\\\n",
    "p(k,j)p(j,i)p(i,k) &= p(k,i)p(i,j)p(j,k)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Conversely, if for an irreducible and ergodic markov chain and any state loop $i, i_1, i_2,...i_k,j, i$ the markov chain satisifes:\n",
    "\n",
    "$$\n",
    "p(i,i_1)p(i_1,i_2)...p(i_k,j)p(j,i) = p(i,j)p(j,i_k)p(i_k,i_{k-1})...p(i_1,i)\n",
    "$$\n",
    "then the $p(i,j)^{k+1} = p(i,i_1)p(i_1,i_2)...p(i_k,j)$ has a limiting distribution $\\pi(j)$ such that $\\lim_{k \\rightarrow \\infty}p(i,j)^{k+1} = \\pi(j)$. Similarly, $p(j,i)^{k+1} = (j,i_k)p(i_k,i_{k-1})...p(i_1,i)$ has a limiting distribution $\\pi(i)$ such that $\\lim_{k \\rightarrow \\infty}p(j,i)^{k+1} = \\pi(i)$, and it concluded:\n",
    "\n",
    "$$\n",
    "\\pi(j)p(j,i)=\\pi(i)p(i,j)\n",
    "$$\n",
    "Therefore $p$ is time-reversible.\n",
    "\n",
    "### Theorem\n",
    "\n",
    "The irreducible ergodic Markov chain with transition probability $p(.,.)$ is time reversible if and only if \n",
    "\n",
    "$$\n",
    "p(i,i_1)p(i_1,i_2)...p(i_k,j)p(j,i) = p(i,j)p(j,i_k)p(i_k,i_{k-1})...p(i_1,i)\n",
    "$$\n",
    "\n",
    "for any states $i, i_1,i_2,...j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Chain Intro\n",
    "\n",
    "**Video lecture: https://youtu.be/2UOnWQHaxD0**\n",
    "\n",
    "Let $\\{X_n, n=1,2,...\\}$ be a Markov chain with transition matrix $p(.|.)$. Suppose a signal $S_n$ is issued each time the Markov chain enters a state at time $n$ and only depends on current state of the Markov chain at time $n$. i.e. \n",
    "$$\n",
    "\\begin{align}\n",
    "&P(S_n=s | X_n=j, X_{n-1}=j_{n-1}, S_{n-1}=s_{n-1}, X_{n-2}=j_{n-2}, S_{n-2}=s_{n-2},...X_1=j_1, S_1=s_1) \\\\\n",
    "&= P(S_n=s|X_n=j) = q(s|j)\n",
    "\\end{align}\n",
    "$$\n",
    "At the end we use $q(s|j)$ to denote such probability.\n",
    "\n",
    "In a hidden Markov chain experiment, an observer is only able to observe the signals $\\mathbf{S^n} = \\{S_n, S_{n-1}...S_1\\}$. The Markov states $\\mathbf{X^n}=\\{X_n, X_{n-1},...X_1\\}$ are unobserved(hidden).\n",
    "We are interested in finding the probablity of $P(X_k=j|\\mathbf{S^n}=\\mathbf{s_n})$ where $k \\le n$ and $\\mathbf{s_n}=[s_1,s_2,...s_n]'$, $P(X_{n+1}=j|\\mathbf{S^n}=\\mathbf{s_n})$, $P(S_{n+1}=s_{n+1}|\\mathbf{S^n} = \\mathbf{s_n})$ and $P(\\mathbf{S^n}=\\mathbf{s_n})$.\n",
    "\n",
    "\n",
    "Let's define $F_n(j)=P(\\mathbf{S^n}=\\mathbf{s_n}, X_n=j)$, if we could find the values of $F_n(j)$, then $P(\\mathbf{S^n}=\\mathbf{s_n}) = \\sum_{j}F_n(j)$, and $P(X_n=j|\\mathbf{S^n}=\\mathbf{s_n}) = \\frac{P(X_n=j, \\mathbf{S^n}=\\mathbf{s_n})}{P(\\mathbf{S^n}=\\mathbf{s_n})} = \\frac{F_n(j)}{\\sum_{j}F_n(j)}$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "F_n(j) &= P(\\mathbf{S^{n-1}}=\\mathbf{s_{n-1}}, S_n=s_n, X_n=j) \\\\\n",
    "&= \\sum_{i} P(\\mathbf{S^{n-1}}=\\mathbf{s_{n-1}}, X_{n-1} = i, S_n=s_n, X_n=j) \\\\\n",
    "&= \\sum_{i} P(X_n=j, S_n=s_n | X_{n-1}=i \\mathbf{S^{n-1}}=\\mathbf{s_{n-1}}) F_{n-1}(i) \\\\\n",
    "&= \\sum_{i} P(X_n=j, S_n=s_n | X_{n-1}=i) F_{n-1}(i) \\\\\n",
    "&= \\sum_{i} P(S_n=s_n | X_n=j, X_{n-1}=i) P(X_n=j | X_{n-1}=i) F_{n-1}(i) \\\\\n",
    "&= \\sum_{i} P(S_n=s_n | X_n=j) P(X_n=j | X_{n-1}=i) F_{n-1}(i) \\\\\n",
    "&= q(s_n|j)\\sum_{i} p(j|i)F_{n-1}(i) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and $F_1(i) = P(X_1=i, S_1=s_1) = q(s_1|i)P(X_1=i)$. $F_n$ is an iterative approach moving forward from $F_1$ to $F_n$ therefore it's called **Forward Approach**.\n",
    "\n",
    "Another way of solving $P(\\mathbf{S^n}=\\mathbf{s_n})$ is to iterative backwards (**Backward Approach**):\n",
    "\n",
    "Define $B_k(i) = P(S_{k+1}=s_{k+1},...,S_n=s_n | X_k=i)$. Then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(\\mathbf{S^n}=\\mathbf{s_n}) &= \\sum_{i}P(\\mathbf{S^n}=\\mathbf{s_n}|X_1=i) P(X_1=i) \\\\\n",
    "&= \\sum_{i}P(S_2=s_2, S_3=s_3,...,S_n=s_n|S_1=s_1, X_1=i) P(S_1=s_1|X_1=i) P(X_1=i) \\\\\n",
    "&= \\sum_{i}B_1(i)q(s_1|i)P(X_1=i)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let's find $B_k(i)$ recursively:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "B_k(i) &= \\sum_{j} P(S_{k+1}=s_{k+1},...,S_n=s_n | X_{k+1} = j, X_k=i) P(X_{k+1} = j | X_i = i) \\\\\n",
    "&= \\sum_{j} P(S_{k+1}=s_{k+1},...,S_n=s_n | X_{k+1} = j) P(X_{k+1} = j | X_i = i) \\\\\n",
    "&= \\sum_{j} P(S_{k+2}=s_{k+2},...,S_n=s_n | S_{k+1} = s_{k+1},X_{k+1} = j)P(S_{k+1} = s_{k+1}|X_{k+1} = j) P(X_{k+1} = j | X_i = i) \\\\\n",
    "&= \\sum_{j} P(S_{k+2}=s_{k+2},...,S_n=s_n | X_{k+1} = j)P(S_{k+1} = s_{k+1}|X_{k+1} = j) P(X_{k+1} = j | X_i = i) \\\\\n",
    "&= \\sum_{j} B_{k+1}(j)q(s_{k+1}|j) p(j |i) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Starting with $B_{n-1}(i) = P(S_n=s_n|X_{n-1}=i) = \\sum_{j}P(S_n=s_n|X_n=j, X_{n-1}=i) p(j|i) = \\sum_{j}q(s_n|j) p(j|i)$.\n",
    "\n",
    "$P(\\mathbf{S^n} = \\mathbf{s_n})$ can also be calculated from both forward and backward iterations as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(\\mathbf{S^n} &= \\mathbf{s_n}) = \\sum_{j} P(\\mathbf{S^n} = \\mathbf{s_n}, X_k=j) \\\\\n",
    "&= \\sum_{j} P(S_{k+1}=s_{k+1},...,S_n=s_n|\\mathbf{S^k}=\\mathbf{s_k}, X_k=j)P(\\mathbf{S^k}=\\mathbf{s_k}, X_k=j) \\\\\n",
    "&= \\sum_{j} B_k(j)F_k(j) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Now let's come back to the unsolved questions:\n",
    "\n",
    "For $k \\le n$, find:\n",
    "$$\n",
    "\\begin{align}\n",
    "P(X_k=j|\\mathbf{S^n}=\\mathbf{s_n}) &= \\frac{P(\\mathbf{S^n} = \\mathbf{s_n}, X_k=j)}{P(\\mathbf{S^n} = \\mathbf{s_n})} \\\\\n",
    "&= \\frac{F_k(j)B_k(j)}{\\sum_{j}F_k(j)B_k(j)}\n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "P(X_{n+1}=j|\\mathbf{S^n}=\\mathbf{s_n}) = \\sum_{i} P(X_{n+1}=j|X_n=i, \\mathbf{S^n}=\\mathbf{s_n}) P(X_n=i | \\mathbf{S^n}=\\mathbf{s_n})\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(S_{n+1}=s_{n+1}|\\mathbf{S^n} = \\mathbf{s_n}) \\sum_{i} P(S_{n+1}=s_{n+1}|X_{n+1}=i, \\mathbf{S^n} = \\mathbf{s_n}) P(X_{n+1}=i| \\mathbf{S^n} = \\mathbf{s_n})\n",
    "$$\n",
    "\n",
    "\n",
    "We could also predict the entire hidden states $\\mathbf{X^n}$. We are interested in finding the states $\\mathbf{i_n} = \\{i_1,...,i_n\\}$ that maximizes $P(\\mathbf{X^n} = \\mathbf{i_n}|\\mathbf{S^n}=\\mathbf{s_n})$:\n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{i^n}}{\\mathrm{argmax}}P(\\mathbf{X^n} = \\mathbf{i_n}|\\mathbf{S^n}=\\mathbf{s_n})  = \\underset{\\mathbf{i^n}}{\\mathrm{argmax}}\\frac{P(\\mathbf{X^n} = \\mathbf{i_n}, \\mathbf{S^n}=\\mathbf{s_n})}{P(\\mathbf{S^n}=\\mathbf{s_n})}\n",
    "$$\n",
    "We are looking for $\\mathbf{i_n}$ that maximizes the above probability which is equivalent to find $\\mathbf{i_n}$ that maximize $\\underset{\\mathbf{i_n}}{\\mathrm{argmax}} P(\\mathbf{X^n} = \\mathbf{i_n}, \\mathbf{S^n}=\\mathbf{s_n})$.\n",
    "\n",
    "\n",
    "Let $\\mathbf{X^k} = \\{X_1,X_2,...,X_k\\}$ be the first $k$ states, $\\mathbf{i_k} = \\{i_1,...,i_k\\}$. And let $V_k$ be:\n",
    "\n",
    "$$\n",
    "V_k(j) = \\underset{\\mathbf{i_{k-1}}}{\\mathrm{max}} {P(\\mathbf{X^{k-1}}=\\mathbf{i_{k-1}}, X_k=j, \\mathbf{S^k}=\\mathbf{s_k})}\n",
    "$$\n",
    "\n",
    "The values of $V_k(j)$ for all $k$ and $j$ can be find recursively as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "V_k(j) &= \\underset{\\mathbf{i_{k-1}}}{\\mathrm{max}} {P(\\mathbf{X^{k-1}}=\\mathbf{i_{k-1}}, X_k=j, \\mathbf{S^k}=\\mathbf{s_k})} \\\\\n",
    "&= \\underset{i_{k-1}}{\\mathrm{max\\:}} \\underset{\\mathbf{i_{k-2}}}{\\mathrm{max\\;}} {P(\\mathbf{X^{k-2}}=\\mathbf{i_{k-2}}, X_{k-1}=i_{k-1}, X_k=j, \\mathbf{S^{k-1}}=\\mathbf{s_{k-1}},S_k=s_k)} \\\\\n",
    "&= \\underset{i_{k-1}}{\\mathrm{max\\:}}  {P(X_k=j,S_k=s_k|\\mathbf{X^{k-2}}=\\mathbf{i_{k-2}}, X_{k-1}=i_{k-1},  \\mathbf{S^{k-1}}=\\mathbf{s_{k-1}})} \\times \\underset{\\mathbf{i_{k-2}}}{\\mathrm{max\\;}} P(\\mathbf{X^{k-2}}=\\mathbf{i_{k-2}}, X_{k-1}=i_{k-1},  \\mathbf{S^{k-1}}=\\mathbf{s_{k-1}}) \\\\\n",
    "&= \\underset{i_{k-1}}{\\mathrm{max\\:}} \\{P(X_k=j,S_k=s_k|X_{k-1}=i_{k-1}) \\times V_{k-1}(i_{k-1}) \\}  \\\\\n",
    "&= q(s_k|j) \\underset{i_{k-1}}{\\mathrm{max\\:}} \\{V_{k-1}(i_{k-1}) p(j|i_{k-1})\\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $V_1$ equals:\n",
    "\n",
    "$$\n",
    "V_1(j) = P(X_1=j, S_1=s_1) = P(X_1=j) q(s_1|j)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Our maximization problem comes down to finding the sequence in a reversed direction by first finding $i_n$ such that:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\underset{\\{i_1,...i_n\\}}{\\mathrm{max}} P(\\mathbf{X^n} = \\mathbf{i_n}, \\mathbf{S^n}=\\mathbf{s_n}) \\\\\n",
    "&=  \\underset{i_n}{\\mathrm{max}} V_n(i_n)\n",
    "\\end{align}\n",
    "$$\n",
    "Suppose $j_n$ is the value of $i_n$ that maximizes $V_n(.)$. i.e. $j_n=\\underset{i_n}{argmax}V_n(i_n)$. But now how do we find $j_{n-1}$ that maximizes $i_{n-1}$?\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "V_n(j_n) &= \\underset{\\mathbf{i_{n-1}}}{max\\:} \\{ P(\\mathbf{X^{n-1}}=\\mathbf{i_{n-1}}, X_n=j_n, \\mathbf{S^n}=\\mathbf{s_n}) \\} \\\\\n",
    "&= q(s_n|j_n) \\underset{i_{n-1}}{\\mathrm{max\\:}} \\{V_{n-1}(i_{n-1}) p(j_n|i_{n-1})\\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The $j_{n-1}$ we are looking for is the value of $i_{n-1}$ such that $V_{n-1}(i_{n-1}) p(j_n|i_{n-1})$ is maximized. i.e. $j_{n-1} = \\underset{i_{n-1}}{\\mathrm{argmax\\:}} \\{V_{n-1}(i_{n-1}) p(j_n|i_{n-1})\\}$.\n",
    "\n",
    "We could iterate likewise to find $j_{n-2}$, the value of $i_{n-2}$ such that $V_{n-2}(i_{n-2}) p(j_{n-1}|i_{n-2})$ is maximized. i.e. $j_{n-2} = \\underset{i_{n-2}}{\\mathrm{argmax\\:}} \\{V_{n-2}(i_{n-2}) p(j_{n-1}|i_{n-2})\\}$.\n",
    "\n",
    "Such algorithm of find the hiddens that maximizes $P(\\mathbf{X^n}=\\mathbf{i_n}|\\mathbf{S^n}=s_n)$ is called **Viterbi Algorithm**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
