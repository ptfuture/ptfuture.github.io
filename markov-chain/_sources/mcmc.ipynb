{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "554aecf8-fefa-417d-8ed4-2b2abab857ac",
   "metadata": {},
   "source": [
    "# Markov Chain Monte Carlo\n",
    "\n",
    "It's not always easy to generate the samples of a random vector $\\mathbf{X}$ especially when $\\mathbf{X}$ has dependent random variables or it's not easy to integrate w.r.t its density function.\n",
    "\n",
    "If we could design a Markov chain where its stationary distribution is the same as the distribution of $\\mathbf{X}$, then we could generate samples of $\\mathbf{X}$ via the markov chain process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eba6f16-a067-42b6-b541-f6fcec60ea66",
   "metadata": {},
   "source": [
    "## Metropolis–Hastings Algorithm\n",
    "\n",
    "**Video lecture: https://youtu.be/-o8drmhudjs**\n",
    "\n",
    "Metropolis–Hastings Algorithm can generate samples directly from any probability distribition $\\pi(x)$, given that $\\pi(x) \\propto f(x)$ where $f(x)$ is known and values of $f(x)$ can be calcuated. Metropolis–Hastings Algorithm solves the problem when integrating $f(x)$ (finding the normalization factor) is computationally difficult.\n",
    "\n",
    "Metropolis–Hastings Algorithm constructs a Markov chain which asymtotically reaches a unique stationary distribution equal to $\\pi$.\n",
    "\n",
    "To guarantee the existence of stationary distribution, Metropolis–Hastings Algorithm constructs the Markov chain $p(y|x)$ (transition probability from $x$ to $y$) such that it's time-reversible. i.e. \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\pi(x)p(y|x) = \\pi(y)p(x|y) \\tag{1}\n",
    "\\end{equation}\n",
    "$$ \n",
    "Because $\\int{\\pi(x)p(y|x)dy} = \\pi(x) = \\int{\\pi(y)p(x|y)dy}$, which indicates that $\\pi$ is a stationary distribution.\n",
    "\n",
    "To guarantee the uniqueness of the stationary distribution, the Markov chain must also be ergodic (aperiodic and positive recurrent).\n",
    "\n",
    "\n",
    "Our goal is to construct a Markov chain $p(.|.)$ such that (1) holds. And Metropolis–Hastings Algorithm's idea is to use another known Markov chain $q(.|.)$  which we know how to generate samples from and use a reject-or-accept ratio say $\\alpha(.|.)$ to make the accepted samples from $q(.|.)$ asymtotically reaches to the stationary distribution $\\pi$. Let the two step procedure's probability construct equals to $p(y|x)$ as follows:\n",
    "\n",
    "$$\n",
    "p(y|x) = q(y|x)\\alpha(y|x)\n",
    "$$\n",
    "\n",
    "Insert into (1) we get:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\pi(x)q(y|x)\\alpha(y|x) = \\pi(y)q(x|y)\\alpha(x|y)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha(y|x) = \\alpha(x|y)\\frac{\\pi(y)q(x|y)}{\\pi(x)q(y|x)} = \\alpha(x|y)\\frac{f(y)q(x|y)}{f(x)q(y|x)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If we let $\\alpha(y|x)$ equals to:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha(y|x) = min\\Big( \\frac{f(y)q(x|y)}{f(x)q(y|x)}, 1  \\Big)     \\tag{2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then (2) makes the equation (1) holds in all cases. Why? if $\\alpha(y|x) = \\frac{\\pi(y)q(x|y)}{\\pi(x)q(y|x)} < 1$ then $\\alpha(x|y) = 1$, equation holds, and vice versa.\n",
    "\n",
    "### Alogorithm:\n",
    "\n",
    "1. Initialization: Let $t = 1$, Pick an initial state $x_1$ and let $X_t = x_1$. Choose a large number $N$ for stopping the markov chain process.\n",
    "2. while $t \\le N$ do:\n",
    "    1. Propose a new sample candidate $Y$ according to $q(Y|X_t)$\n",
    "    2. Calculate the acceptance probability $\\alpha(Y|X_t) = min\\Big( \\frac{f(Y)q(X_t|Y)}{f(X)q(Y|X_y)}, 1  \\Big)$\n",
    "    3. Generate $U \\sim U(0,1)$\n",
    "    4. If $U \\le \\alpha(Y|X_t)$, accept the sample $Y$ by $t = t+1$ setting $X_{t+1} = Y$\n",
    "    5. Else continue to the next loop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765c163b-d341-4880-b086-1a464310f594",
   "metadata": {},
   "source": [
    "## Gibbs Sampler\n",
    "\n",
    "**Video lecture: https://youtu.be/wlpDrBEZXpg**\n",
    "\n",
    "Gibbs sampler makes generating high dimension samples easier than Metropolis–Hastings Algorithm. The idea is to pick a component of the vector and propose a new sample by generating it from conditional distribution given other components fixed.\n",
    "\n",
    "Gibbs Sampler algorithm is another version of Metropolis–Hastings Algorithm with the proposal distribution $q$ as follows:\n",
    "\n",
    "$$\n",
    "q(\\mathbf{y}|\\mathbf{x}) = \\frac{1}{n}f(y_i|x_1,...,x_{i-1},x_{i+1},...,x_{n}) = \\frac{f(\\mathbf{y})}{ n\\int{ f(\\mathbf{x}) dx_i } }\n",
    "$$\n",
    "\n",
    "Note that for fixed $i$, $\\int{ f(\\mathbf{x}) dx_i } = \\int{ f(\\mathbf{y}) dy_i }$,\n",
    "\n",
    "$$\n",
    "\\frac{f(\\mathbf{y})q(\\mathbf{x}|\\mathbf{y})}{f(\\mathbf{x})q(\\mathbf{y}|\\mathbf{x})} = \\frac{f(\\mathbf{y})f(\\mathbf{x})}{f(\\mathbf{x})f(\\mathbf{y})} = 1\n",
    "$$\n",
    "\n",
    "The \\@ref(eq:mcmc-mh-alpha) now becomes constant 1: $\\alpha(\\mathbf{y}|\\mathbf{x}) = min\\Big( 1, 1  \\Big) = 1$.\n",
    "\n",
    "### Alogorithm:\n",
    "\n",
    "1. Initialization: Let $j = 1$, Pick an initial state $\\mathbf{x_1}$ and let $\\mathbf{X_j} = \\mathbf{x_1}$. Choose a large number $N$ for stopping the markov chain process.\n",
    "2. For $j = 1$ to $N$ do:\n",
    "    1. For $i = 1$ to $n$ do:\n",
    "        1. Draw $Y_i$ from conditional pdf $f(y_i|x_{j,1},...x_{j,i-1},x_{j,i+1},...,x_{j,n})$\n",
    "    2. $\\mathbf{X_{j+1}} = Y$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
