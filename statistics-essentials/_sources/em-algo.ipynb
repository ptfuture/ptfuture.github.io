{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0154a058-6f55-440a-a725-62cd4f684596",
   "metadata": {},
   "source": [
    "# Expectationâ€“maximization algorithm\n",
    "\n",
    "**Video lecture: https://youtu.be/GEdAP680w5Y**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e656d80-061a-47a2-84c8-e8d58a896a04",
   "metadata": {},
   "source": [
    "## Problem Desription\n",
    "\n",
    "Let $X' = [X_1, X_2,..., X_{n}]$ be observed sample, iid with pdf $f(x|\\theta)$. Let $Z'=[Z_1, Z_2,..., Z_{n}]$ be hidden/unobserved. We further assume that $\\{ X_i \\}$ and $\\{ Z_j \\}$ are mutually independent.\n",
    "\n",
    "Let $g(\\mathbf{x}|\\theta)$ be the joint pdf of $\\mathbf{X}$. \n",
    "\n",
    "Let $h(\\mathbf{x},\\mathbf{z}|\\theta)$ be the complete pdf of $\\mathbf{X}, \\mathbf{Z}$. \n",
    "\n",
    "Let $k(\\mathbf{z}|\\theta,\\mathbf{x})$ be the conditional pdf of the hidden data given the observed data.\n",
    "\n",
    "$$\n",
    "k(\\mathbf{z}|\\theta,\\mathbf{x}) = \\frac{h(\\mathbf{x},\\mathbf{z}|\\theta)}{g(\\mathbf{x}|\\theta)}\n",
    "$$\n",
    "\n",
    "We want to find an estimate of the unknown parameter $\\theta$.\n",
    "\n",
    "We'll conduct MLE to maximize the probability of observing $X$  by maximizing $g(\\mathbf{x}|\\theta)$. However, the missing data $\\{ Z_i \\}$ yields $g(\\mathbf{x}|\\theta)$ with more unknown parameters.\n",
    "\n",
    "McLachlan and Krishman (1997) gives a in-depth knowledge of an algorithm call EM Algo. Let's see the inspiration of EM algo. We want to maximize **incomplete likelyhood function** $L(\\theta; \\mathbf{x})=g(\\mathbf{x}|\\theta)$ wrt $\\theta$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\log{L(\\theta; \\mathbf{x})} = \\log{g(\\mathbf{x}|\\theta)} &= \\int{\\log{g(\\mathbf{x}|\\theta)} k(\\mathbf{z}|\\theta_0,\\mathbf{x}) d\\mathbf{z} } \\\\\n",
    "  &= \\int{[\\log{h(\\mathbf{x}, \\mathbf{z}|\\theta) - \\log{k(\\mathbf{z}|\\theta, \\mathbf{x})}}] k(\\mathbf{z}|\\theta_0,\\mathbf{x}) } d\\mathbf{z} \\\\\n",
    "  &= \\int{\\log{h(\\mathbf{x}, \\mathbf{z}|\\theta) k(\\mathbf{z}|\\theta_0,\\mathbf{x}) } d\\mathbf{z}  - \\int{[\\log{k(\\mathbf{z}|\\theta, \\mathbf{x})}}] k(\\mathbf{z}|\\theta_0,\\mathbf{x}) } d\\mathbf{z} \\\\\n",
    "  &= E_{\\mathbf{Z}}[\\log{h(\\mathbf{x},\\mathbf{Z}|\\theta)|\\theta_0,\\mathbf{x}}] - E_{\\mathbf{Z}}[\\log{k(\\mathbf{Z}|\\theta, \\mathbf{x})}|\\theta_0,\\mathbf{x}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here we are assuming $\\theta_0$ is known. The $L(\\theta; \\mathbf{x}, \\mathbf{z}) = h(\\mathbf{x},\\mathbf{Z}|\\theta)|\\theta_0,\\mathbf{x}$ is the **complete likelyhood function**.\n",
    "\n",
    "Let \n",
    "\n",
    "$$\n",
    "Q(\\theta|\\theta_0,\\mathbf{x})=E_{\\mathbf{Z}}[\\log{h(\\mathbf{x},\\mathbf{Z}|\\theta)|\\theta_0,\\mathbf{x}}]\n",
    "$$ \n",
    "\n",
    "And let \n",
    "\n",
    "$$\n",
    "R(\\theta|\\theta_0,\\mathbf{x})=E_{\\mathbf{Z}}[\\log{k(\\mathbf{Z}|\\theta, \\mathbf{x})}|\\theta_0,\\mathbf{x}]\n",
    "$$\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "**[Step E] Expectation Step**\n",
    "\n",
    "Suppose $\\hat{\\theta}_m$ is known. Compute $Q(\\theta|\\hat{\\theta}_m,\\mathbf{x})=E_{\\mathbf{Z}}[\\log{h(\\mathbf{x},\\mathbf{z}|\\theta)|\\hat{\\theta}_m,\\mathbf{x}}]$\n",
    "\n",
    "**[Step M] Maximization Step**\n",
    "\n",
    "Find next $\\hat{\\theta}_{m+1}=\\underset{\\theta}{argmax}{Q(\\theta|\\hat{\\theta}_m,\\mathbf{x})}$\n",
    "\n",
    "**[Step S] Stop Step**\n",
    "\n",
    "Stop the iteration when $\\|Q(\\theta_{m+1}|\\hat{\\theta}_m,\\mathbf{x}) - Q(\\theta_m|\\hat{\\theta}_m,\\mathbf{x}) \\| \\le \\epsilon$\n",
    "\n",
    "\n",
    "**Why Only Maximizing $Q(\\theta|\\hat{\\theta}_m,\\mathbf{x})$?**\n",
    "\n",
    "## Theorem\n",
    "\n",
    "Let $\\{\\theta_m\\}$ be the sequence of estimations by conducting the EM Algorithm.\n",
    "\n",
    "$$\n",
    "R(\\theta_{m+1}|\\theta_m,\\mathbf{x}) \\le R(\\theta_m|\\theta_m,\\mathbf{x})\n",
    "$$\n",
    "\n",
    "Proof:\n",
    "\n",
    "We need to prove: \n",
    "\n",
    "$$\n",
    "E_{\\mathbf{Z}}[\\log{k(\\mathbf{Z}|\\theta_{m+1}, \\mathbf{x})}|\\theta_m,\\mathbf{x}] - E_{\\mathbf{Z}}[\\log{k(\\mathbf{Z}|\\theta_m, \\mathbf{x})}|\\theta_m,\\mathbf{x}] \\le 0\n",
    "$$\n",
    "\n",
    "that is, by Jensen's inequality:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  E_\\mathbf{Z}\\bigg[ \\log{\\frac{k(\\mathbf{Z}|\\theta_{m+1}, \\mathbf{x})}{k(\\mathbf{Z}|\\theta_{m}, \\mathbf{x})}} \\bigg| \\theta_m,\\mathbf{x} \\bigg] &\\le \\log{E_\\mathbf{Z}\\bigg[ \\frac{k(\\mathbf{Z}|\\theta_{m+1}, \\mathbf{x})}{k(\\mathbf{Z}|\\theta_{m}, \\mathbf{x})} \\bigg| \\theta_m,\\mathbf{x} \\bigg]} \\\\\n",
    "  &= \\log \\int{\\frac{k(\\mathbf{Z}|\\theta_{m+1}, \\mathbf{x})}{k(\\mathbf{Z}|\\theta_{m}, \\mathbf{x})}k(\\mathbf{Z}|\\theta_{m}, \\mathbf{x})}d\\mathbf{z} = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "## Gaussian mixture\n",
    "\n",
    "Let $\\mathbf{x} = (\\mathbf{x}_1,\\mathbf{x}_2,\\ldots,\\mathbf{x}_n)$ be a sample of $n$ independent observations from a mixture of two multivariate normal distributions of dimension $d$, and let $\\mathbf {z} =(z_{1},z_{2},\\ldots ,z_{n})$ be the latent variables that determine the component from which the observation originates.\n",
    "\n",
    "$X_{i}\\mid (Z_{i}=1)\\sim {\\mathcal {N}}_{d}({\\mathbf {\\mu }}_{1},\\Sigma _{1})$ and $X_{i}\\mid (Z_{i}=2)\\sim {\\mathcal {N}}_{d}({\\mathbf {\\mu }}_{2},\\Sigma _{2})$ \n",
    "\n",
    "where $\\operatorname{P} (Z_i = 1 ) = \\tau_1$ and $\\operatorname {P} (Z_{i}=2)=\\tau _{2}=1-\\tau _{1}$\n",
    "\n",
    "The aim is to estimate the unknown parameters representing the mixing value between the Gaussians and the means and covariances of each:\n",
    "\n",
    "$$\n",
    "\\theta ={\\big (}{\\mathbf {\\tau }},{\\mathbf {\\mu }}_{1},{\\mathbf {\\mu }}_{2},\\Sigma _{1},\\Sigma _{2}{\\big )}\n",
    "$$\n",
    "\n",
    "Deriving the pdf of X:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  P(X \\le x)' &= P(X \\le x | Z = 1)' P(Z = 1) + P(X \\le x | Z = 2)' P(Z = 2) \\\\\n",
    "              &= \\tau _{1}\\ f(\\mathbf {x};{\\mathbf {\\mu }}_{1},\\Sigma _{1}) + \\tau _{2}\\ f(\\mathbf {x};{\\mathbf {\\mu }}_{2},\\Sigma _{2})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "where the incomplete-data likelihood function is:\n",
    "\n",
    "$$\n",
    "L(\\theta ;\\mathbf {x} )=\\prod _{i=1}^{n}\\sum _{j=1}^{2}\\tau _{j}\\ f(\\mathbf {x} _{i};{\\mathbf {\\mu }}_{j},\\Sigma _{j})\n",
    "$$\n",
    "\n",
    "and the complete-data likelihood function is:\n",
    "\n",
    "$$\n",
    "L(\\theta ;\\mathbf {x} ,\\mathbf {z} )=p(\\mathbf {x} ,\\mathbf {z} \\mid \\theta )=\\prod _{i=1}^{n}\\prod _{j=1}^{2}\\ [f(\\mathbf {x} _{i};{\\mathbf {\\mu }}_{j},\\Sigma _{j})\\tau _{j}]^{\\mathbb {I} (z_{i}=j)}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "L(\\theta ;\\mathbf {x} ,\\mathbf {z} )=\\exp \\left\\{\\sum _{i=1}^{n}\\sum _{j=1}^{2}\\mathbb {I} (z_{i}=j){\\big [}\\log \\tau _{j}-{\\tfrac {1}{2}}\\log |\\Sigma _{j}|-{\\tfrac {1}{2}}(\\mathbf {x} _{i}-{\\mathbf {\\mu }}_{j})^{\\top }\\Sigma _{j}^{-1}(\\mathbf {x} _{i}-{\\mathbf {\\mu }}_{j})-{\\tfrac {d}{2}}\\log(2\\pi ){\\big ]}\\right\\}\n",
    "$$\n",
    "\n",
    "The conditional pdf of $k(\\mathbf{Z}|\\theta, \\mathbf{x})$ is:\n",
    "\n",
    "$$\n",
    "T_{j,i}^{(t)}:=\\operatorname {P} (Z_{i}=j\\mid X_{i}=\\mathbf {x} _{i};\\theta ^{(t)})={\\frac {\\tau _{j}^{(t)}\\ f(\\mathbf {x} _{i};{\\mathbf {\\mu }}_{j}^{(t)},\\Sigma _{j}^{(t)})}{\\tau _{1}^{(t)}\\ f(\\mathbf {x} _{i};{\\mathbf {\\mu }}_{1}^{(t)},\\Sigma _{1}^{(t)})+\\tau _{2}^{(t)}\\ f(\\mathbf {x} _{i};{\\mathbf {\\mu }}_{2}^{(t)},\\Sigma _{2}^{(t)})}}\n",
    "$$\n",
    "\n",
    "**E Step**\n",
    "\n",
    "We could derive $Q(\\theta \\mid \\theta ^{(t)})$ as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q(\\theta \\mid \\theta ^{(t)})&=\\operatorname {E} _{\\mathbf {Z} \\mid \\mathbf {X} ,\\mathbf {\\theta } ^{(t)}}[\\log L(\\theta ;\\mathbf {x} ,\\mathbf {Z} )]\\\\\n",
    "  &=\\operatorname {E} _{\\mathbf {Z} \\mid \\mathbf {X} ,\\mathbf {\\theta } ^{(t)}}[\\log \\prod _{i=1}^{n}L(\\theta ;\\mathbf {x} _{i},Z_{i})]\\\\\n",
    "  &=\\operatorname {E} _{\\mathbf {Z} \\mid \\mathbf {X} ,\\mathbf {\\theta } ^{(t)}}[\\sum _{i=1}^{n}\\log L(\\theta ;\\mathbf {x} _{i},Z_{i})]\\\\\n",
    "  &=\\sum _{i=1}^{n}\\operatorname {E} _{Z_{i}\\mid \\mathbf {X} ;\\mathbf {\\theta } ^{(t)}}[\\log L(\\theta ;\\mathbf {x} _{i},Z_{i})]\\\\\n",
    "  &=\\sum _{i=1}^{n}\\sum _{j=1}^{2}P(Z_{i}=j\\mid X_{i}=\\mathbf {x} _{i};\\theta ^{(t)})\\log L(\\theta _{j};\\mathbf {x} _{i},j)\\\\\n",
    "  &=\\sum _{i=1}^{n}\\sum _{j=1}^{2}T_{j,i}^{(t)}{\\big [}\\log \\tau _{j}-{\\tfrac {1}{2}}\\log |\\Sigma _{j}|-{\\tfrac {1}{2}}(\\mathbf {x} _{i}-{\\mathbf {\\mu }}_{j})^{\\top }\\Sigma _{j}^{-1}(\\mathbf {x} _{i}-{\\mathbf {\\mu }}_{j})-{\\tfrac {d}{2}}\\log(2\\pi ){\\big ]}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**M Step**\n",
    "\n",
    "The linear form of $\\tau_{i}$ and $\\mu_{i}/\\Sigma_i$ means we could maiximize $Q$ independently by maximizing $\\tau_{i}$ and $\\mu_{i}/\\Sigma_{i}$ terms separately.\n",
    "\n",
    "$\\tau$ terms, with constrain $\\tau_2 = 1 - \\tau_1$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\mathbf {\\tau }}^{(t+1)} &= {\\underset {\\mathbf {\\tau }}{\\operatorname {arg\\,max} }}\\ Q(\\theta \\mid \\theta ^{(t)})\\\\\n",
    "                              &={\\underset {\\mathbf {\\tau }}{\\operatorname {arg\\,max} }}\\ \\left\\{\\left[\\sum _{i=1}^{n}T_{1,i}^{(t)}\\right]\\log \\tau _{1}+\\left[\\sum _{i=1}^{n}T_{2,i}^{(t)}\\right]\\log \\tau _{2}\\right\\}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "By taking derivative of each $\\tau$, we get:\n",
    "\n",
    "\n",
    "$$\n",
    "\\tau _{j}^{(t+1)}={\\frac {\\sum _{i=1}^{n}T_{j,i}^{(t)}}{\\sum _{i=1}^{n}(T_{1,i}^{(t)}+T_{2,i}^{(t)})}}={\\frac {1}{n}}\\sum _{i=1}^{n}T_{j,i}^{(t)}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "For the next estimates of $(\\mu_1, \\Sigma_1)$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "({\\mathbf {\\mu }}_{1}^{(t+1)},\\Sigma_{1}^{(t+1)}) &= {\\underset { {\\mathbf{\\mu}}_{1},\\Sigma_{1} }{\\operatorname {arg\\,max} }}  Q(\\theta \\mid \\theta ^{(t)})\\\\\n",
    "&= {\\underset { {\\mathbf{\\mu}}_{1},\\Sigma_{1} }{\\operatorname {arg\\,max} }} \\sum _{i=1}^{n}T_{1,i}^{(t)}\\left\\{   -{\\tfrac {1}{2}}\\log |\\Sigma _{1}|-{\\tfrac {1}{2}}(\\mathbf {x} _{i}-{\\mathbf {\\mu }}_{1})^{\\top }\\Sigma_{1}^{-1}(\\mathbf {x} _{i}-{\\mathbf {\\mu }}_{1})  \\right\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To derive the formula for $\\mu_{i}^{(t+1)}$, we'll gonna make use of this formula: $\\mathbf{ \\frac{\\partial w^T A w}{\\partial w} = 2Aw}$, where A is symmetric.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  0 &= \\frac{\\partial{Q(\\theta \\mid \\theta ^{(t)})}}{\\partial{\\mu_1}} \\\\\n",
    "    &= \\sum_{i=1}^{n} -T_{1,i}^{(t)} \\Sigma_{1}^{-1} (\\mathbf {x} _{i}-{\\mathbf {\\mu }}_{1})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{\\mu}_1^{(t+1)} = \\frac{\\sum_{i=1}^n T_{1,i}^{(t)} \\mathbf{x}_i}{\\sum_{i=1}^n T_{1,i}^{(t)}} \\\\   \n",
    "\\mathbf{\\mu}_2^{(t+1)} = \\frac{\\sum_{i=1}^n T_{2,i}^{(t)} \\mathbf{x}_i}{\\sum_{i=1}^n T_{2,i}^{(t)}} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "To derive the formula for $\\Sigma_{i}^{(t+1)}$, we'll gonna make use of the following equations:\n",
    "\n",
    "- Invariant under cyclic permutation $tr[ABC] = tr[CAB] = tr[BCA]$\n",
    "\n",
    "- Derivative of a trace: $\\frac{\\partial}{\\partial{A}} tr[BA] = B^T$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\frac{\\partial}{\\partial{a_{ij}}} tr[BA] &= \\frac{\\partial}{\\partial{a_{ij}}} \\sum_{k} \\sum_{l} b_{kl}a_{lk} = b_{ji} \\\\\n",
    "  \\frac{\\partial}{\\partial{A}} tr[BA] &= B^T\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- Derivative of a log determinant: $\\frac{\\partial}{\\partial{A}}\\log{|A|} = A^{-T}$. \n",
    "  \n",
    "  To establish the result, note that: $\\frac{\\partial}{\\partial{a_{ij}}}\\log{|A|} = \\frac{1}{|A|} \\frac{\\partial}{\\partial{a_{ij}}} |A|$, and recall the formula for the matrix inverse: $A^{-1} = \\frac{1}{|A|} C^T$, therefore $A^{-T} = \\frac{1}{|A|} C$. All we need to prove is $\\frac{\\partial}{\\partial{a_{ij}}} |A| = C$, However, $|A| = \\sum_j{(-1)^{(i+j)}a_{ij}M_{ij}}$,\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial}{\\partial{a_{ij}}}|A| = \\frac{\\partial}{\\partial{a_{ij}}}\\left\\{\\sum_j{(-1)^{(i+j)}a_{ij}M_{ij}}\\right\\} = (-1)^{(i+j)} M_{ij}\n",
    "  $$\n",
    "\n",
    "  Which is exactly the cofactor matrix $C$ at entry $(i,j)$. Therefore, $\\frac{\\partial}{\\partial{a_{ij}}} |A| = C$.\n",
    "\n",
    "- Determinant of an inverse is the inverse of the determinant: $|A|^{-1} = |A^{-1}|$\n",
    "\n",
    "Now we have all the formulas we need to derive the $\\Sigma_1^{(t+1)}$.\n",
    "\n",
    "Because $\\frac{\\partial}{\\partial{A}} x^tAx = \\frac{\\partial}{\\partial{A}} tr[x^tAx] = \\frac{\\partial}{\\partial{A}} tr[xx^tA] = xx^T$, we get:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  0 &= \\frac{\\partial{Q(\\theta \\mid \\theta ^{(t)})}}{\\partial{\\Sigma_1^{-1}}} \\\\\n",
    "    &=\\sum _{i=1}^{n}\\sum _{j=1}^{2}T_{j,i}^{(t)}{\\bigg [} \\frac{\\partial}{\\partial{\\Sigma_1^{-1}}} {\\tfrac {1}{2}}\\log |\\Sigma _{j}^{-1}| -  \\frac{\\partial}{\\partial{\\Sigma_1^{-1}}} tr {\\big [} {\\tfrac {1}{2}}(\\mathbf {x} _{i}-{\\mathbf {\\mu }}_{j})^{\\top }\\Sigma _{j}^{-1}(\\mathbf {x} _{i}-{\\mathbf {\\mu }}_{j}) {\\big ]} {\\bigg ]} \\\\\n",
    "    &=\\sum _{i=1}^{n} T_{1,i}^{(t)} \\tfrac{1}{2} \\Sigma_1 - \\sum _{i=1}^{n} T_{1,i}^{(t)} {\\tfrac {1}{2}}(\\mathbf {x} _{i}-{\\mathbf {\\mu }}_{1})(\\mathbf {x} _{i}-{\\mathbf {\\mu }}_{1})^{\\top }\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\Sigma_1^{(t+1)} &= \\frac{\\sum_{i=1}^n T_{1,i}^{(t)} (\\mathbf{x}_i - \\mathbf{\\mu}_1^{(t+1)}) (\\mathbf{x}_i - \\mathbf{\\mu}_1^{(t+1)})^\\top }{\\sum_{i=1}^n T_{1,i}^{(t)}} \\\\\n",
    "\\Sigma _{2}^{(t+1)} &={\\frac {\\sum _{i=1}^{n}T_{2,i}^{(t)}(\\mathbf {x} _{i}-{\\mathbf {\\mu }}_{2}^{(t+1)})(\\mathbf {x} _{i}-{\\mathbf {\\mu }}_{2}^{(t+1)})^{\\top }}{\\sum _{i=1}^{n}T_{2,i}^{(t)}}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
