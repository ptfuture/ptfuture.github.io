{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum likelihood estimator (MLE) is method of finding the estimators of unknow parameters that maximized the probability of the know outcome samples.\n",
    "\n",
    "That is, what value of $\\theta$ that gave us the maximum probability (If it's continuous, then think of it $dx$ as very small, that is the instantanuous probability of getting an exact outcome of $x_1, x_2,...$)\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\underset{\\theta}{\\text{argmax}} L(\\theta, \\mathbf{X})\n",
    "$$\n",
    "\n",
    "Where $L(\\theta, \\mathbf{X}) = \\prod{f(\\theta, x_i)}$\n",
    "\n",
    "Equivalently,  $\\ell(\\theta, \\mathbf{X})=\\log{L}=\\sum_{i=1}^n \\log{f(\\theta,x_i)}$, most cases, dealing with log is much easier.\n",
    "\n",
    "That is to find the $\\theta$ where the $\\text{D}_\\theta(\\prod{f(\\theta, x_i)}) = 0$ and $\\text{D}_{\\theta}^2(\\prod{f(\\theta, x_i)})$ is negative definite.\n",
    "\n",
    "__The problem is how good is this MLE?__\n",
    "__How close is our estimate to the real value $\\theta_0$__\n",
    "\n",
    "## __Theorem__ \n",
    "\n",
    "\n",
    "$\\hat{\\theta}_n \\xrightarrow{p} \\theta_0$ in probability.\n",
    "\n",
    "\n",
    "_Proof:_\n",
    "\n",
    "(Brief) It can be proved that $\\lim\\limits_{n \\rightarrow \\infty} P_{\\theta_0}[ \\ell(\\theta_0, \\mathbf{X}) > \\ell(\\theta, \\mathbf{X})] = 1$ for all $\\theta \\ne \\theta_0$ That is $\\theta_0$ yields the largest $L$ value in probability when $n \\rightarrow \\infty$ \n",
    "\n",
    "What we need to  prove here is $\\lim\\limits_{n \\rightarrow \\infty} P[\\lvert \\hat{\\theta}_n - \\theta_0 \\rvert < \\epsilon] = 1$, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "S_1 & = \\lbrace\\mathbf{X}: \\ell(\\theta_0, \\mathbf{X}) > \\max(\\ell(\\theta_0 - \\frac{\\epsilon}{2}, \\mathbf{X}), \\ell(\\theta_0 + \\frac{\\epsilon}{2}, \\mathbf{X}))\\rbrace \\\\\n",
    "S_2 & = \\lbrace\\mathbf{X}: \\lvert\\hat{\\theta}_n(\\mathbf{X}) - \\theta_0 \\rvert < \\epsilon \\rbrace\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If $\\mathbf{X} \\in S_1$, then $\\theta_0 - \\frac{\\epsilon}{2} \\le \\hat{\\theta}_n(\\mathbf{X}) \\le \\theta_0 + \\frac{\\epsilon}{2}$, thus $\\mathbf{X} \\in S_2$, hence, $S_1 \\subseteq S_2$. We proved it $1 = \\lim\\limits_{n \\rightarrow \\infty} P[S1] \\le P[S_2] \\le 1$\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "__What about the variance of MLE?__\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "1 & = \\int_{-\\infty}^{+\\infty} f(x, \\theta) dx \\\\\n",
    "0 & = \\int_{-\\infty}^{+\\infty} \\frac{\\partial{f(x, \\theta)/\\partial{\\theta}}}{f(x, \\theta)}f(x, \\theta) dx \\\\\n",
    "0 & = \\int_{-\\infty}^{+\\infty} \\frac{\\partial{\\log{f(x, \\theta)}}}{ \\partial{\\theta}} f(x, \\theta) dx\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "0 = E\\bigg[\\frac{\\partial{\\log{f(X, \\theta)}}}{ \\partial{\\theta}}\\bigg]  \\tag{1}\n",
    "$$\n",
    "\n",
    "In multiparameter case:\n",
    "\n",
    "$$\n",
    "0 = E[\\nabla\\log f(X, \\mathbf{\\theta})] = \\begin{bmatrix}\n",
    "  E\\bigg[ \\frac{\\partial\\log f(X, \\theta_1)}{\\partial{\\theta_1}} \\bigg] \\\\\n",
    "  E\\bigg[ \\frac{\\partial\\log f(X, \\theta_2)}{\\partial{\\theta_2}} \\bigg] \\\\\n",
    "  ... \\\\\n",
    "  E\\bigg[ \\frac{\\partial\\log f(X, \\theta_p)}{\\partial{\\theta_p}} \\bigg] \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "By taking the second derivative of (1), we get\n",
    "\n",
    "\n",
    "$$\n",
    "I(\\theta) = Var\\bigg(\\frac{\\partial{\\log{f(x, \\theta)}}}{ \\partial{\\theta}}\\bigg) = \\int_{-\\infty}^{+\\infty} \\bigg(\\frac{\\partial{\\log{f(x, \\theta)}}}{ \\partial{\\theta}}\\bigg)^2 f(x, \\theta) dx = - \\int_{-\\infty}^{+\\infty} \\frac{\\partial^2{\\log{f(x, \\theta)}}}{ \\partial{\\theta^2}} f(x, \\theta) dx \\tag{2}\n",
    "$$\n",
    "\n",
    "In multiparameter case:\n",
    "\n",
    "$$\n",
    "I(\\theta) = Cov(\\nabla\\log f(X, \\mathbf{\\theta})) = \\bigg[ I_{jk} \\bigg]\n",
    "$$\n",
    "\n",
    "where $I_{jk}$ is:\n",
    "\n",
    "$$\n",
    "I_{jk}(\\theta) = -E\\Big[ \\frac{\\partial^2\\log f(X, \\theta)}{\\partial\\theta_j \\partial\\theta_k} \\Big]\n",
    "$$\n",
    "\n",
    "\n",
    "__(2) is called the Fisher Information__\n",
    "\n",
    "If $\\mathbf{X}$ is sample of size $n$ iid. Then its information is the sum of those independent fisher information \n",
    "\n",
    "$$\n",
    "I_n(\\mathbf{\\theta}) = Var\\bigg(\\frac{\\partial\\log L(\\theta, \\mathbf{X})}{\\theta}\\bigg) = Var\\bigg(\\sum^n_{i=1}\\frac{\\partial\\log{f(X_i,\\theta)}}{\\partial\\theta}\\bigg) = nI(\\theta)\n",
    "$$\n",
    "\n",
    "In multiparameter case:\n",
    "\n",
    "Let $Z_i = \\sum^n_{i=1} \\frac{\\partial\\log{f(X_i, \\mathbf{\\theta})}}{\\partial\\theta_i}$, we could derive that $E[Z_i] = 0$ and\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  I_n(\\theta) &= Cov\\begin{bmatrix} \n",
    "  Z_1 \\\\\n",
    "  Z_2 \\\\\n",
    "  ... \\\\\n",
    "  Z_p\n",
    "  \\end{bmatrix} \\\\\n",
    "  & = \\begin{bmatrix}\n",
    "    E[Z_1Z_1], E[Z_1Z_2], ... E[Z_1Z_p] \\\\\n",
    "    E[Z_1Z_2], E[Z_1Z_1], ... E[Z_2Z_p] \\\\\n",
    "    ... \\\\\n",
    "    E[Z_pZ_1], E[Z_pZ_2], ... E[Z_pZ_p]\n",
    "  \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "It can be calculated that:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "I_n^{jk}(\\theta) &= E[Z_jZ_k] \\\\\n",
    "                 &= -E\\bigg[ \\sum^n_{i=1} \\frac{\\partial\\log{f(X_i, \\theta)}}{\\partial\\theta_j\\partial\\theta_k} \\bigg] \\\\\n",
    "                 &= -nE\\bigg[\\frac{\\partial\\log{f(X_i, \\theta)}}{\\partial\\theta_j\\partial\\theta_k} \\bigg]\n",
    "\\end{aligned}\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "## __Rao-Cramer Lower Bound Theorem__\n",
    "\n",
    "$X_1, X_2,...,X_n$ iid with pdf $f(x,\\theta)$\n",
    "Let $Y = \\mu(X_1, X_2,..., X_n)$ be a statistic with $E[Y]=\\kappa(\\theta)$\n",
    "\n",
    "$$\n",
    "Var(Y) \\ge \\frac{[\\kappa'(\\theta)]^2}{nI(\\theta)}\n",
    "$$\n",
    "\n",
    "If $Y$ is unbiased estimator, $\\kappa(\\theta) = \\theta$, then,\n",
    "\n",
    "$$\n",
    "Var(Y) \\ge \\frac{1}{nI(\\theta)}\n",
    "$$\n",
    "\n",
    "**For multivariant case:**\n",
    "\n",
    "Let $Y$ be a statistic:\n",
    "\n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "  \\mu_1(\\{X_i\\}) \\\\\n",
    "  \\mu_2(\\{X_i\\}) \\\\\n",
    "  ... \\\\\n",
    "  \\mu_p(\\{X_i\\})\n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "with $E[Y]$ be:\n",
    "\n",
    "$$\n",
    "E[Y]=\\begin{bmatrix}\n",
    "  \\kappa_1(\\theta) \\\\\n",
    "  \\kappa_2(\\theta) \\\\\n",
    "  ... \\\\\n",
    "  \\kappa_p(\\theta) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And the first derivative of $E[Y]$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  D(E[Y]) = D(\\kappa) &= \\begin{bmatrix}\n",
    "    \\frac{\\partial\\kappa_1(\\theta)}{\\partial\\theta_1} \\frac{\\partial\\kappa_1(\\theta)}{\\partial\\theta_2} ... \\frac{\\partial\\kappa_1(\\theta)}{\\partial\\theta_p} \\\\\n",
    "    \\frac{\\partial\\kappa_2(\\theta)}{\\partial\\theta_1} \\frac{\\partial\\kappa_2(\\theta)}{\\partial\\theta_2} ... \\frac{\\partial\\kappa_2(\\theta)}{\\partial\\theta_p} \\\\\n",
    "    ... \\\\\n",
    "    \\frac{\\partial\\kappa_p(\\theta)}{\\partial\\theta_1} \\frac{\\partial\\kappa_p(\\theta)}{\\partial\\theta_2} ... \\frac{\\partial\\kappa_p(\\theta)}{\\partial\\theta_p} \\\\\n",
    "  \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  Cov_\\theta(Y) &\\ge \\frac{1}{n} D(\\kappa) I_n(\\theta)^{-1} D(\\kappa)^T\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If $Y$ is an unbaised estimate of $\\{\\theta_i\\}$, then:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  Cov_\\theta(Y) &\\ge \\frac{1}{n} I_n(\\theta)^{-1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "## Theorem\n",
    "\n",
    "Assume $X_1, X_2,...X_n$ iid with pdf $f(x,\\theta_0)$, $\\hat{\\theta}_n \\xrightarrow{P} \\theta_0$.\n",
    "\n",
    "Given that $\\log f(x, \\theta)$ is three times differentiable and for all $\\theta \\in \\Omega$, $\\exists \\epsilon \\in \\mathbf{R}$ and a function $M(x)$ such that $\\bigg\\lvert \\frac{\\partial^3 \\log f(x, \\theta)}{\\partial\\theta^3} \\bigg\\rvert \\le M(x)$, with $E_{\\theta_0}[M(X)] < \\infty$ for all $\\theta \\in (\\theta_0 - \\epsilon, \\theta_0 + \\epsilon)$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\xrightarrow{D} N\\bigg(0, \\frac{1}{I(\\theta_0)}\\bigg)\n",
    "$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
