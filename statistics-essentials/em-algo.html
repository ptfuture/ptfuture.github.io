
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Expectation–maximization algorithm &#8212; Statistics Essentials</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Quadratic Forms" href="quadratic-forms.html" />
    <link rel="prev" title="Maximum Likelihood Estimator" href="mle.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Statistics Essentials</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="mle.html">
   Maximum Likelihood Estimator
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Expectation–maximization algorithm
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="quadratic-forms.html">
   Quadratic Forms
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/em-algo.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#problem-desription">
   Problem Desription
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#algorithm">
   Algorithm
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#theorem">
   Theorem
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gaussian-mixture">
   Gaussian mixture
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="expectationmaximization-algorithm">
<h1>Expectation–maximization algorithm<a class="headerlink" href="#expectationmaximization-algorithm" title="Permalink to this headline">¶</a></h1>
<p><strong>Video lecture: <a class="reference external" href="https://youtu.be/GEdAP680w5Y">https://youtu.be/GEdAP680w5Y</a></strong></p>
<div class="section" id="problem-desription">
<h2>Problem Desription<a class="headerlink" href="#problem-desription" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(X' = [X_1, X_2,..., X_{n}]\)</span> be observed sample, iid with pdf <span class="math notranslate nohighlight">\(f(x|\theta)\)</span>. Let <span class="math notranslate nohighlight">\(Z'=[Z_1, Z_2,..., Z_{n}]\)</span> be hidden/unobserved. We further assume that <span class="math notranslate nohighlight">\(\{ X_i \}\)</span> and <span class="math notranslate nohighlight">\(\{ Z_j \}\)</span> are mutually independent.</p>
<p>Let <span class="math notranslate nohighlight">\(g(\mathbf{x}|\theta)\)</span> be the joint pdf of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(h(\mathbf{x},\mathbf{z}|\theta)\)</span> be the complete pdf of <span class="math notranslate nohighlight">\(\mathbf{X}, \mathbf{Z}\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(k(\mathbf{z}|\theta,\mathbf{x})\)</span> be the conditional pdf of the hidden data given the observed data.</p>
<div class="math notranslate nohighlight">
\[
k(\mathbf{z}|\theta,\mathbf{x}) = \frac{h(\mathbf{x},\mathbf{z}|\theta)}{g(\mathbf{x}|\theta)}
\]</div>
<p>We want to find an estimate of the unknown parameter <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>We’ll conduct MLE to maximize the probability of observing <span class="math notranslate nohighlight">\(X\)</span>  by maximizing <span class="math notranslate nohighlight">\(g(\mathbf{x}|\theta)\)</span>. However, the missing data <span class="math notranslate nohighlight">\(\{ Z_i \}\)</span> yields <span class="math notranslate nohighlight">\(g(\mathbf{x}|\theta)\)</span> with more unknown parameters.</p>
<p>McLachlan and Krishman (1997) gives a in-depth knowledge of an algorithm call EM Algo. Let’s see the inspiration of EM algo. We want to maximize <strong>incomplete likelyhood function</strong> <span class="math notranslate nohighlight">\(L(\theta; \mathbf{x})=g(\mathbf{x}|\theta)\)</span> wrt <span class="math notranslate nohighlight">\(\theta\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
  \log{L(\theta; \mathbf{x})} = \log{g(\mathbf{x}|\theta)} &amp;= \int{\log{g(\mathbf{x}|\theta)} k(\mathbf{z}|\theta_0,\mathbf{x}) d\mathbf{z} } \\
  &amp;= \int{[\log{h(\mathbf{x}, \mathbf{z}|\theta) - \log{k(\mathbf{z}|\theta, \mathbf{x})}}] k(\mathbf{z}|\theta_0,\mathbf{x}) } d\mathbf{z} \\
  &amp;= \int{\log{h(\mathbf{x}, \mathbf{z}|\theta) k(\mathbf{z}|\theta_0,\mathbf{x}) } d\mathbf{z}  - \int{[\log{k(\mathbf{z}|\theta, \mathbf{x})}}] k(\mathbf{z}|\theta_0,\mathbf{x}) } d\mathbf{z} \\
  &amp;= E_{\mathbf{Z}}[\log{h(\mathbf{x},\mathbf{Z}|\theta)|\theta_0,\mathbf{x}}] - E_{\mathbf{Z}}[\log{k(\mathbf{Z}|\theta, \mathbf{x})}|\theta_0,\mathbf{x}]
\end{aligned}
\end{split}\]</div>
<p>Here we are assuming <span class="math notranslate nohighlight">\(\theta_0\)</span> is known. The <span class="math notranslate nohighlight">\(L(\theta; \mathbf{x}, \mathbf{z}) = h(\mathbf{x},\mathbf{Z}|\theta)|\theta_0,\mathbf{x}\)</span> is the <strong>complete likelyhood function</strong>.</p>
<p>Let</p>
<div class="math notranslate nohighlight">
\[
Q(\theta|\theta_0,\mathbf{x})=E_{\mathbf{Z}}[\log{h(\mathbf{x},\mathbf{Z}|\theta)|\theta_0,\mathbf{x}}]
\]</div>
<p>And let</p>
<div class="math notranslate nohighlight">
\[
R(\theta|\theta_0,\mathbf{x})=E_{\mathbf{Z}}[\log{k(\mathbf{Z}|\theta, \mathbf{x})}|\theta_0,\mathbf{x}]
\]</div>
</div>
<div class="section" id="algorithm">
<h2>Algorithm<a class="headerlink" href="#algorithm" title="Permalink to this headline">¶</a></h2>
<p><strong>[Step E] Expectation Step</strong></p>
<p>Suppose <span class="math notranslate nohighlight">\(\hat{\theta}_m\)</span> is known. Compute <span class="math notranslate nohighlight">\(Q(\theta|\hat{\theta}_m,\mathbf{x})=E_{\mathbf{Z}}[\log{h(\mathbf{x},\mathbf{z}|\theta)|\hat{\theta}_m,\mathbf{x}}]\)</span></p>
<p><strong>[Step M] Maximization Step</strong></p>
<p>Find next <span class="math notranslate nohighlight">\(\hat{\theta}_{m+1}=\underset{\theta}{argmax}{Q(\theta|\hat{\theta}_m,\mathbf{x})}\)</span></p>
<p><strong>[Step S] Stop Step</strong></p>
<p>Stop the iteration when <span class="math notranslate nohighlight">\(\|Q(\theta_{m+1}|\hat{\theta}_m,\mathbf{x}) - Q(\theta_m|\hat{\theta}_m,\mathbf{x}) \| \le \epsilon\)</span></p>
<p><strong>Why Only Maximizing <span class="math notranslate nohighlight">\(Q(\theta|\hat{\theta}_m,\mathbf{x})\)</span>?</strong></p>
</div>
<div class="section" id="theorem">
<h2>Theorem<a class="headerlink" href="#theorem" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\{\theta_m\}\)</span> be the sequence of estimations by conducting the EM Algorithm.</p>
<div class="math notranslate nohighlight">
\[
R(\theta_{m+1}|\theta_m,\mathbf{x}) \le R(\theta_m|\theta_m,\mathbf{x})
\]</div>
<p>Proof:</p>
<p>We need to prove:</p>
<div class="math notranslate nohighlight">
\[
E_{\mathbf{Z}}[\log{k(\mathbf{Z}|\theta_{m+1}, \mathbf{x})}|\theta_m,\mathbf{x}] - E_{\mathbf{Z}}[\log{k(\mathbf{Z}|\theta_m, \mathbf{x})}|\theta_m,\mathbf{x}] \le 0
\]</div>
<p>that is, by Jensen’s inequality:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
  E_\mathbf{Z}\bigg[ \log{\frac{k(\mathbf{Z}|\theta_{m+1}, \mathbf{x})}{k(\mathbf{Z}|\theta_{m}, \mathbf{x})}} \bigg| \theta_m,\mathbf{x} \bigg] &amp;\le \log{E_\mathbf{Z}\bigg[ \frac{k(\mathbf{Z}|\theta_{m+1}, \mathbf{x})}{k(\mathbf{Z}|\theta_{m}, \mathbf{x})} \bigg| \theta_m,\mathbf{x} \bigg]} \\
  &amp;= \log \int{\frac{k(\mathbf{Z}|\theta_{m+1}, \mathbf{x})}{k(\mathbf{Z}|\theta_{m}, \mathbf{x})}k(\mathbf{Z}|\theta_{m}, \mathbf{x})}d\mathbf{z} = 0
\end{aligned}
\end{split}\]</div>
</div>
<div class="section" id="gaussian-mixture">
<h2>Gaussian mixture<a class="headerlink" href="#gaussian-mixture" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{x} = (\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_n)\)</span> be a sample of <span class="math notranslate nohighlight">\(n\)</span> independent observations from a mixture of two multivariate normal distributions of dimension <span class="math notranslate nohighlight">\(d\)</span>, and let <span class="math notranslate nohighlight">\(\mathbf {z} =(z_{1},z_{2},\ldots ,z_{n})\)</span> be the latent variables that determine the component from which the observation originates.</p>
<p><span class="math notranslate nohighlight">\(X_{i}\mid (Z_{i}=1)\sim {\mathcal {N}}_{d}({\mathbf {\mu }}_{1},\Sigma _{1})\)</span> and <span class="math notranslate nohighlight">\(X_{i}\mid (Z_{i}=2)\sim {\mathcal {N}}_{d}({\mathbf {\mu }}_{2},\Sigma _{2})\)</span></p>
<p>where <span class="math notranslate nohighlight">\(\operatorname{P} (Z_i = 1 ) = \tau_1\)</span> and <span class="math notranslate nohighlight">\(\operatorname {P} (Z_{i}=2)=\tau _{2}=1-\tau _{1}\)</span></p>
<p>The aim is to estimate the unknown parameters representing the mixing value between the Gaussians and the means and covariances of each:</p>
<div class="math notranslate nohighlight">
\[
\theta ={\big (}{\mathbf {\tau }},{\mathbf {\mu }}_{1},{\mathbf {\mu }}_{2},\Sigma _{1},\Sigma _{2}{\big )}
\]</div>
<p>Deriving the pdf of X:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
  P(X \le x)' &amp;= P(X \le x | Z = 1)' P(Z = 1) + P(X \le x | Z = 2)' P(Z = 2) \\
              &amp;= \tau _{1}\ f(\mathbf {x};{\mathbf {\mu }}_{1},\Sigma _{1}) + \tau _{2}\ f(\mathbf {x};{\mathbf {\mu }}_{2},\Sigma _{2})
\end{aligned}
\end{split}\]</div>
<p>where the incomplete-data likelihood function is:</p>
<div class="math notranslate nohighlight">
\[
L(\theta ;\mathbf {x} )=\prod _{i=1}^{n}\sum _{j=1}^{2}\tau _{j}\ f(\mathbf {x} _{i};{\mathbf {\mu }}_{j},\Sigma _{j})
\]</div>
<p>and the complete-data likelihood function is:</p>
<div class="math notranslate nohighlight">
\[
L(\theta ;\mathbf {x} ,\mathbf {z} )=p(\mathbf {x} ,\mathbf {z} \mid \theta )=\prod _{i=1}^{n}\prod _{j=1}^{2}\ [f(\mathbf {x} _{i};{\mathbf {\mu }}_{j},\Sigma _{j})\tau _{j}]^{\mathbb {I} (z_{i}=j)}
\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[
L(\theta ;\mathbf {x} ,\mathbf {z} )=\exp \left\{\sum _{i=1}^{n}\sum _{j=1}^{2}\mathbb {I} (z_{i}=j){\big [}\log \tau _{j}-{\tfrac {1}{2}}\log |\Sigma _{j}|-{\tfrac {1}{2}}(\mathbf {x} _{i}-{\mathbf {\mu }}_{j})^{\top }\Sigma _{j}^{-1}(\mathbf {x} _{i}-{\mathbf {\mu }}_{j})-{\tfrac {d}{2}}\log(2\pi ){\big ]}\right\}
\]</div>
<p>The conditional pdf of <span class="math notranslate nohighlight">\(k(\mathbf{Z}|\theta, \mathbf{x})\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
T_{j,i}^{(t)}:=\operatorname {P} (Z_{i}=j\mid X_{i}=\mathbf {x} _{i};\theta ^{(t)})={\frac {\tau _{j}^{(t)}\ f(\mathbf {x} _{i};{\mathbf {\mu }}_{j}^{(t)},\Sigma _{j}^{(t)})}{\tau _{1}^{(t)}\ f(\mathbf {x} _{i};{\mathbf {\mu }}_{1}^{(t)},\Sigma _{1}^{(t)})+\tau _{2}^{(t)}\ f(\mathbf {x} _{i};{\mathbf {\mu }}_{2}^{(t)},\Sigma _{2}^{(t)})}}
\]</div>
<p><strong>E Step</strong></p>
<p>We could derive <span class="math notranslate nohighlight">\(Q(\theta \mid \theta ^{(t)})\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
Q(\theta \mid \theta ^{(t)})&amp;=\operatorname {E} _{\mathbf {Z} \mid \mathbf {X} ,\mathbf {\theta } ^{(t)}}[\log L(\theta ;\mathbf {x} ,\mathbf {Z} )]\\
  &amp;=\operatorname {E} _{\mathbf {Z} \mid \mathbf {X} ,\mathbf {\theta } ^{(t)}}[\log \prod _{i=1}^{n}L(\theta ;\mathbf {x} _{i},Z_{i})]\\
  &amp;=\operatorname {E} _{\mathbf {Z} \mid \mathbf {X} ,\mathbf {\theta } ^{(t)}}[\sum _{i=1}^{n}\log L(\theta ;\mathbf {x} _{i},Z_{i})]\\
  &amp;=\sum _{i=1}^{n}\operatorname {E} _{Z_{i}\mid \mathbf {X} ;\mathbf {\theta } ^{(t)}}[\log L(\theta ;\mathbf {x} _{i},Z_{i})]\\
  &amp;=\sum _{i=1}^{n}\sum _{j=1}^{2}P(Z_{i}=j\mid X_{i}=\mathbf {x} _{i};\theta ^{(t)})\log L(\theta _{j};\mathbf {x} _{i},j)\\
  &amp;=\sum _{i=1}^{n}\sum _{j=1}^{2}T_{j,i}^{(t)}{\big [}\log \tau _{j}-{\tfrac {1}{2}}\log |\Sigma _{j}|-{\tfrac {1}{2}}(\mathbf {x} _{i}-{\mathbf {\mu }}_{j})^{\top }\Sigma _{j}^{-1}(\mathbf {x} _{i}-{\mathbf {\mu }}_{j})-{\tfrac {d}{2}}\log(2\pi ){\big ]}.
\end{aligned}
\end{split}\]</div>
<p><strong>M Step</strong></p>
<p>The linear form of <span class="math notranslate nohighlight">\(\tau_{i}\)</span> and <span class="math notranslate nohighlight">\(\mu_{i}/\Sigma_i\)</span> means we could maiximize <span class="math notranslate nohighlight">\(Q\)</span> independently by maximizing <span class="math notranslate nohighlight">\(\tau_{i}\)</span> and <span class="math notranslate nohighlight">\(\mu_{i}/\Sigma_{i}\)</span> terms separately.</p>
<p><span class="math notranslate nohighlight">\(\tau\)</span> terms, with constrain <span class="math notranslate nohighlight">\(\tau_2 = 1 - \tau_1\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
{\mathbf {\tau }}^{(t+1)} &amp;= {\underset {\mathbf {\tau }}{\operatorname {arg\,max} }}\ Q(\theta \mid \theta ^{(t)})\\
                              &amp;={\underset {\mathbf {\tau }}{\operatorname {arg\,max} }}\ \left\{\left[\sum _{i=1}^{n}T_{1,i}^{(t)}\right]\log \tau _{1}+\left[\sum _{i=1}^{n}T_{2,i}^{(t)}\right]\log \tau _{2}\right\}.
\end{aligned}
\end{split}\]</div>
<p>By taking derivative of each <span class="math notranslate nohighlight">\(\tau\)</span>, we get:</p>
<div class="math notranslate nohighlight">
\[
\tau _{j}^{(t+1)}={\frac {\sum _{i=1}^{n}T_{j,i}^{(t)}}{\sum _{i=1}^{n}(T_{1,i}^{(t)}+T_{2,i}^{(t)})}}={\frac {1}{n}}\sum _{i=1}^{n}T_{j,i}^{(t)}
\]</div>
<p>For the next estimates of <span class="math notranslate nohighlight">\((\mu_1, \Sigma_1)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
({\mathbf {\mu }}_{1}^{(t+1)},\Sigma_{1}^{(t+1)}) &amp;= {\underset { {\mathbf{\mu}}_{1},\Sigma_{1} }{\operatorname {arg\,max} }}  Q(\theta \mid \theta ^{(t)})\\
&amp;= {\underset { {\mathbf{\mu}}_{1},\Sigma_{1} }{\operatorname {arg\,max} }} \sum _{i=1}^{n}T_{1,i}^{(t)}\left\{   -{\tfrac {1}{2}}\log |\Sigma _{1}|-{\tfrac {1}{2}}(\mathbf {x} _{i}-{\mathbf {\mu }}_{1})^{\top }\Sigma_{1}^{-1}(\mathbf {x} _{i}-{\mathbf {\mu }}_{1})  \right\}
\end{aligned}
\end{split}\]</div>
<p>To derive the formula for <span class="math notranslate nohighlight">\(\mu_{i}^{(t+1)}\)</span>, we’ll gonna make use of this formula: <span class="math notranslate nohighlight">\(\mathbf{ \frac{\partial w^T A w}{\partial w} = 2Aw}\)</span>, where A is symmetric.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
  0 &amp;= \frac{\partial{Q(\theta \mid \theta ^{(t)})}}{\partial{\mu_1}} \\
    &amp;= \sum_{i=1}^{n} -T_{1,i}^{(t)} \Sigma_{1}^{-1} (\mathbf {x} _{i}-{\mathbf {\mu }}_{1})
\end{aligned}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{\mu}_1^{(t+1)} = \frac{\sum_{i=1}^n T_{1,i}^{(t)} \mathbf{x}_i}{\sum_{i=1}^n T_{1,i}^{(t)}} \\   
\mathbf{\mu}_2^{(t+1)} = \frac{\sum_{i=1}^n T_{2,i}^{(t)} \mathbf{x}_i}{\sum_{i=1}^n T_{2,i}^{(t)}} 
\end{aligned}
\end{split}\]</div>
<p>To derive the formula for <span class="math notranslate nohighlight">\(\Sigma_{i}^{(t+1)}\)</span>, we’ll gonna make use of the following equations:</p>
<ul class="simple">
<li><p>Invariant under cyclic permutation <span class="math notranslate nohighlight">\(tr[ABC] = tr[CAB] = tr[BCA]\)</span></p></li>
<li><p>Derivative of a trace: <span class="math notranslate nohighlight">\(\frac{\partial}{\partial{A}} tr[BA] = B^T\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
  \frac{\partial}{\partial{a_{ij}}} tr[BA] &amp;= \frac{\partial}{\partial{a_{ij}}} \sum_{k} \sum_{l} b_{kl}a_{lk} = b_{ji} \\
  \frac{\partial}{\partial{A}} tr[BA] &amp;= B^T
\end{aligned}
\end{split}\]</div>
<ul>
<li><p>Derivative of a log determinant: <span class="math notranslate nohighlight">\(\frac{\partial}{\partial{A}}\log{|A|} = A^{-T}\)</span>.</p>
<p>To establish the result, note that: <span class="math notranslate nohighlight">\(\frac{\partial}{\partial{a_{ij}}}\log{|A|} = \frac{1}{|A|} \frac{\partial}{\partial{a_{ij}}} |A|\)</span>, and recall the formula for the matrix inverse: <span class="math notranslate nohighlight">\(A^{-1} = \frac{1}{|A|} C^T\)</span>, therefore <span class="math notranslate nohighlight">\(A^{-T} = \frac{1}{|A|} C\)</span>. All we need to prove is <span class="math notranslate nohighlight">\(\frac{\partial}{\partial{a_{ij}}} |A| = C\)</span>, However, <span class="math notranslate nohighlight">\(|A| = \sum_j{(-1)^{(i+j)}a_{ij}M_{ij}}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
  \frac{\partial}{\partial{a_{ij}}}|A| = \frac{\partial}{\partial{a_{ij}}}\left\{\sum_j{(-1)^{(i+j)}a_{ij}M_{ij}}\right\} = (-1)^{(i+j)} M_{ij}
  \]</div>
<p>Which is exactly the cofactor matrix <span class="math notranslate nohighlight">\(C\)</span> at entry <span class="math notranslate nohighlight">\((i,j)\)</span>. Therefore, <span class="math notranslate nohighlight">\(\frac{\partial}{\partial{a_{ij}}} |A| = C\)</span>.</p>
</li>
<li><p>Determinant of an inverse is the inverse of the determinant: <span class="math notranslate nohighlight">\(|A|^{-1} = |A^{-1}|\)</span></p></li>
</ul>
<p>Now we have all the formulas we need to derive the <span class="math notranslate nohighlight">\(\Sigma_1^{(t+1)}\)</span>.</p>
<p>Because <span class="math notranslate nohighlight">\(\frac{\partial}{\partial{A}} x^tAx = \frac{\partial}{\partial{A}} tr[x^tAx] = \frac{\partial}{\partial{A}} tr[xx^tA] = xx^T\)</span>, we get:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
  0 &amp;= \frac{\partial{Q(\theta \mid \theta ^{(t)})}}{\partial{\Sigma_1^{-1}}} \\
    &amp;=\sum _{i=1}^{n}\sum _{j=1}^{2}T_{j,i}^{(t)}{\bigg [} \frac{\partial}{\partial{\Sigma_1^{-1}}} {\tfrac {1}{2}}\log |\Sigma _{j}^{-1}| -  \frac{\partial}{\partial{\Sigma_1^{-1}}} tr {\big [} {\tfrac {1}{2}}(\mathbf {x} _{i}-{\mathbf {\mu }}_{j})^{\top }\Sigma _{j}^{-1}(\mathbf {x} _{i}-{\mathbf {\mu }}_{j}) {\big ]} {\bigg ]} \\
    &amp;=\sum _{i=1}^{n} T_{1,i}^{(t)} \tfrac{1}{2} \Sigma_1 - \sum _{i=1}^{n} T_{1,i}^{(t)} {\tfrac {1}{2}}(\mathbf {x} _{i}-{\mathbf {\mu }}_{1})(\mathbf {x} _{i}-{\mathbf {\mu }}_{1})^{\top }
\end{aligned}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
  \Sigma_1^{(t+1)} &amp;= \frac{\sum_{i=1}^n T_{1,i}^{(t)} (\mathbf{x}_i - \mathbf{\mu}_1^{(t+1)}) (\mathbf{x}_i - \mathbf{\mu}_1^{(t+1)})^\top }{\sum_{i=1}^n T_{1,i}^{(t)}} \\
\Sigma _{2}^{(t+1)} &amp;={\frac {\sum _{i=1}^{n}T_{2,i}^{(t)}(\mathbf {x} _{i}-{\mathbf {\mu }}_{2}^{(t+1)})(\mathbf {x} _{i}-{\mathbf {\mu }}_{2}^{(t+1)})^{\top }}{\sum _{i=1}^{n}T_{2,i}^{(t)}}}
\end{aligned}
\end{split}\]</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="mle.html" title="previous page">Maximum Likelihood Estimator</a>
    <a class='right-next' id="next-link" href="quadratic-forms.html" title="next page">Quadratic Forms</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Alex Lew<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>