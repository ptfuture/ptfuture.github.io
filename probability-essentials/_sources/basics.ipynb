{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties\n",
    "\n",
    "**Video lecture: https://youtu.be/kpXg2PDRPO4**\n",
    "\n",
    "### Unqiueness of Moment Generating Function\n",
    "\n",
    "Let the CDF of $\\boldsymbol{X}, \\boldsymbol{Y}$ are $F_{\\boldsymbol{X}}(x)$ and $F_{\\boldsymbol{Y}}(y)$, MGF are $M_{\\boldsymbol{X}}(t)=E[e^{t{\\boldsymbol{X}}}]$ and $M_{\\boldsymbol{Y}}(t)=E[e^{t{\\boldsymbol{Y}}}]$. Then $F_{\\boldsymbol{X}}(z)=F_{\\boldsymbol{Y}(z)}$ for all $z\\in R$ iff $M_{\\boldsymbol{X}}(t)=M_{\\boldsymbol{Y}}(t)$ for all $t\\in (-h,h)$ for some $h \\gt 0$.\n",
    "\n",
    "\n",
    "### Independence By MGF\n",
    "\n",
    "$X = [X_1, X_2]'$ are independent iff $M(\\boldsymbol{t}) = E[e^{\\boldsymbol{t}'\\boldsymbol{X}}] = M_{X_1}(t_1)M_{X_2}(t_2)$\n",
    "\n",
    "\n",
    "_Proof:_\n",
    "\n",
    "$X = [X_1, X_2]'$ independent, then $M(\\boldsymbol{t}) \\implies E[e^{\\boldsymbol{t}'\\boldsymbol{X}}] = M_{X_1}(t_1)M_{X_2}(t_2)$. \n",
    "\n",
    "$M(\\boldsymbol{t}) \\impliedby E[e^{\\boldsymbol{t}'\\boldsymbol{X}}] = M_{X_1}(t_1)M_{X_2}(t_2)$, by uniqueness of MGF, $f(x,y)$ of $M(\\boldsymbol{t})$ equals $f_xf_y$ of $M_{X_1}(t_1)M_{X_2}(t_2)$\n",
    "\n",
    "### Pearson correlation coefficient between -1 and 1\n",
    "\n",
    "$\\rho = \\frac{Cov(X,Y)}{\\sigma_{\\boldsymbol{X}}\\sigma_{\\boldsymbol{Y}}} \\in [-1, 1]$\n",
    "\n",
    "\n",
    "_Proof:_\n",
    "\n",
    "By Holder's inequality (Generalization of Cauch Schwarz inequality) in $L^2$ space: $|Cov(X,Y)| \\le \\sigma_{\\boldsymbol{X}}\\sigma_{\\boldsymbol{Y}}$\n",
    "\n",
    "### Commonly used Properties and Formulas\n",
    "\n",
    "1. $Cov(cX, Y) = cCov(X,Y)$\n",
    "2. $Cov(X+Y, Z) = Cov(X, Z) + Cov(Y, Z) = Cov(X, Y+Z)$\n",
    "3. $\\displaystyle Cov(\\sum_{i=1}^n{X_i}, \\sum_{j=1}^m{Y_j}) = \\sum_{i=1}^n\\sum_{j=1}^mCov(X_i, Y_j)$\n",
    "4. $\\displaystyle Var(\\sum_{i=1}^nX_i) = Cov(\\sum_{i=1}^nX_i, \\sum_{j=1}^nX_j) = \\sum_{i=1}^{n}Var(X_i) + 2\\sum_{i=1}^n\\sum_{j\\gt{i}} Cov(X_i, Y_j)$\n",
    "   If $X_i$ are has common $\\mu, \\sigma^2$, then $\\displaystyle Var(\\sum_{i=1}^nX_i) = n\\sigma^2 + n(n-1)\\rho \\sigma^2$\n",
    "5. If $X_1, X_2, ..., X_n$ are iid, then, $Cov(\\bar{X}, X_i-\\bar{X}) = 0$.\n",
    "   Proof: \n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "     \\displaystyle Cov(\\bar{X}, X_i-\\bar{X}) &= Cov(\\bar{X}, X_i) - Cov(\\bar{X}, \\bar{X}) \\\\\n",
    "      &= \\frac{1}{n}Cov(X_i + \\sum_{j\\ne{i}}X_j, X_i) - Var(\\bar{X})\n",
    "      &= \\frac{\\sigma^2}{n} - \\frac{\\sigma^2}{n} = 0\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "6. If $Z ~ N(0,1)$, then $Z^2 \\sim \\chi^2(1)$. Calculate $P(Z^2 \\le z)$. Same, if $Z_1, Z_2,..., Z_n$ are iid standard normal, then $\\displaystyle\\sum_{i=1}^nZ_i^2 \\sim \\chi^2(n)$\n",
    "7. $E_Y[E_X[X|Y]] = \\int\\int{xf(x|y)} g_Y(y) dxdy = \\int\\int{x\\frac{f(x,y)}{f_Y(y)}} f_Y(y) dxdy = \\int\\int{xf(x,y)} dxdy = E_X[X]$\n",
    "8. $\\displaystyle Var[X] = E_Y[Var[X|Y]] + Var[E_X[X|Y]]$, $Var[E_X[X|Y]] \\le Var[X]$\n",
    "    Proof:\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "      E[Var[X|Y]] &= E[E[X^2|Y]] - E[E[X|Y]^2] = E[X^2] - E[E[X|Y]^2] \\\\\n",
    "      Var[E[X|Y]] &= E[E[X|Y]^2] - E[X]^2\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "## Uniform $u(a,b)$\n",
    "\n",
    "1. $f(x)=\\frac{1}{b-a}$\n",
    "2. $\\phi(t) = \\frac{e^{tb}-e^{ta}}{t(b-a)}$\n",
    "3. $\\mu=\\frac{a+b}{2}$\n",
    "4. $\\sigma^2=\\frac{(b-a)^2}{12}$\n",
    "\n",
    "## Binomial $b(n,p)$\n",
    "\n",
    "1. $p(x)=\\binom{n}{x}p^x(1-p)^{n-x}$ where $x=0,1,2,\\dots,n$\n",
    "2. $\\phi(t)=(pe^t+(1-p))^n$\n",
    "3. $\\,u = np$\n",
    "4. $\\sigma^2=np(1-p)$\n",
    "\n",
    "## Geometric $g(p)$\n",
    "\n",
    "1. $p(k)=p(1-p)^{k-1}$ where $k=1,2,\\dots$\n",
    "2. $\\phi(t) =\\frac{pe^t}{1-(1-p)e^t}$\n",
    "3. $\\mu=\\frac{1}{p}$\n",
    "4. $\\sigma^2=\\frac{1-p}{p^2}$\n",
    "\n",
    "## Poisson $\\rho(\\lambda)$\n",
    "\n",
    "1. $p(x)=e^{-\\lambda}\\frac{\\lambda^x}{x!}$ where $x=0,1,2,\\dots$\n",
    "2. $\\phi(t)=e^{\\lambda(e^t-1)}$\n",
    "3. $\\mu=\\lambda$\n",
    "4. $\\sigma^2=\\lambda$\n",
    "\n",
    "Poisson usually used to model probability of $x$ changes in each interval of of length $h$ with the following postulates :\n",
    "\n",
    "1. $g(1, h)=\\rho h + o(h)$, $h \\gt 0$, $\\rho > 0$ constant. $\\rho$ is called poisson constant.\n",
    "2. $\\sum_{x=2}^\\infty g(x,h)=o(h)$\n",
    "3. The number of changes in non-overlapping intervals are independent.\n",
    "4. $g(x, 0) = 0$\n",
    "\n",
    "It can be proven that $g(0, h)=e^{-\\rho h}$, $g(x,h)=\\frac{(\\rho h)^xe^{-\\rho h}}{x!}$ for $x \\gt 0$\n",
    "\n",
    "### Theorem\n",
    "Let $X_1, X_2,\\dots,X_n$ iid $\\sim \\rho(\\lambda_i)$, then $Y=\\sum_{i=1}^nX_i \\sim \\rho(\\sum_{i=1}^n\\lambda_i)$\n",
    "\n",
    "Proof: Use MGF.\n",
    "\n",
    "## Gamma $\\Gamma(\\alpha, \\beta)$\n",
    "\n",
    "1. $f(x) = \\frac{x^{\\alpha-1}e^{-\\frac{x}{\\beta}}}{\\Gamma(\\alpha-1)\\beta^{\\alpha}}$, where $0 \\lt x \\lt \\infty$\n",
    "2. $\\phi(t) = \\frac{1}{(1-\\beta t)^\\alpha}$, where $t \\lt \\frac{1}{\\beta}$\n",
    "3. $\\mu = \\alpha\\beta$\n",
    "4. $\\sigma^2 = \\alpha\\beta^2$\n",
    "\n",
    "Let $W$ be the time that is needed to observe exactly $\\alpha$ events, assuming the poisson postulates also holds for those events($\\lim\\limits_{h \\to 0}\\frac{g(1, h)}{h} = \\rho$).\n",
    "\n",
    "$P(W \\gt w)$ means there are less than $\\alpha$ events occurred in the time interval of $w$. That is $P(W \\gt w) = \\sum_{k=1}^{\\alpha-1}\\frac{(\\rho w)^ke^{-\\rho w}}{k!}$\n",
    "\n",
    "Then we derive the Gamma distribution as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(W \\le w) & = 1 - P(W \\gt w) = 1 - \\sum_{k=1}^{\\alpha-1}\\frac{(\\rho w)^ke^{-\\rho w}}{k!}  \\\\\n",
    "           & = 1 - \\int_{\\rho w}^\\infty \\frac{x^{\\alpha-1}e^{-x}}{\\Gamma(\\alpha-1)}dx \\\\\n",
    "           & = \\int_{-\\infty}^{\\rho w}\\frac{x^{\\alpha-1}e^{-x}}{\\Gamma(\\alpha-1)}dx \\\\\n",
    "           & = \\int_{-\\infty}^w \\frac{(\\rho y)^{\\alpha-1}e^{-\\rho y}}{\\Gamma(\\alpha-1)}\\rho dy \\\\\n",
    "           & = \\int_{-\\infty}^w \\frac{\\rho^{\\alpha}y^{\\alpha-1}e^{-\\rho y}}{\\Gamma(\\alpha-1)} dy \\\\\n",
    "           & = \\int_{-\\infty}^w \\frac{y^{\\alpha-1}e^{-\\frac{y}{\\beta}}}{\\Gamma(\\alpha-1)\\beta^{\\alpha}} dy\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note: let $\\beta = \\frac{1}{\\rho}$\n",
    "\n",
    "\n",
    "\n",
    "## Exponential $e(\\lambda)$\n",
    "\n",
    "When $\\alpha = 1$, we derives exponential distribution with $\\lambda = \\rho$, where $\\rho$ is the poisson constant.\n",
    "\n",
    "1. $f(x)=\\lambda e^{- \\lambda x}$ where $x > 0$\n",
    "2. $\\phi(t) = \\frac{\\lambda}{\\lambda-t}$\n",
    "3. $\\mu=\\frac{1}{\\lambda}$\n",
    "4. $\\sigma^2=\\frac{1}{\\lambda^2}$\n",
    "\n",
    "\n",
    "## Chi-square $\\chi^2(r)$\n",
    "\n",
    "When $\\beta=2$, $\\alpha=\\frac{r}{2}$, we got $\\chi^2(r)$ distribution.\n",
    "\n",
    "1. $f(x)=\\frac{x^{\\frac{r}{2}-1}e^{-\\frac{x}{2}}}{\\Gamma(\\frac{r}{2}-1)2^{\\frac{2}{2}}}$\n",
    "2. $\\phi(t) = \\frac{1}{(1-2 t)^\\frac{r}{2}}$, where $t \\lt \\frac{1}{2}$\n",
    "3. $\\mu = r$\n",
    "4. $\\sigma^2 = 2r$\n",
    "\n",
    "## Beta $\\beta(\\theta, \\zeta)$\n",
    "\n",
    "1. $f(x)=\\frac{x^{\\theta + \\zeta -1}e^{-x}}{\\Gamma(\\theta + \\zeta)}$\n",
    "2. $\\phi(t)$\n",
    "3. $\\mu=\\frac{\\theta}{\\theta+\\zeta}$\n",
    "4. $\\sigma^2=\\frac{\\theta\\zeta}{(\\theta+\\zeta+1)(\\theta+\\zeta)^2}$\n",
    "\n",
    "## Univariate normal\n",
    "\n",
    "### Density function\n",
    "$$\n",
    "f(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}\n",
    "$$\n",
    "\n",
    "\n",
    "## Student Theorem\n",
    "\n",
    "Let $X_1, X_2, \\dots, X_n$ iid $N(\\mu, \\sigma^2)$. Then:\n",
    "\n",
    "1. $\\bar{X} \\sim N(\\mu, \\frac{\\sigma^2}{n})$\n",
    "2. $\\bar{X}$ and $S^2$ are independent\n",
    "3. $\\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2(n-1)$\n",
    "4. $T=\\frac{\\frac{(\\bar{X}-\\mu)}{\\frac{\\sigma}{\\sqrt{n}}}}{\\sqrt{\\frac{(n-1)S^2}{\\sigma^2(n-1)}}} =\\frac{\\sqrt{n}(\\bar{X}-\\mu)}{S} \\sim t(n-1)$\n",
    "\n",
    "_Proof:_\n",
    "\n",
    "1. Prove by MGF, or by the fact that linear combination of iid normals are also normal with $E[\\sum{\\frac{X_i}{n}}] = n\\frac{\\mu}{n}$ and $Var(\\sum{\\frac{X_i}{n}})=n\\frac{\\sigma^2}{n^2}$\n",
    "2. Use the fact that $\\bar{X}$ and ${X_1-\\bar{X}, X_2-\\bar{X},...,X_n-\\bar{X}}$ are all normal and the fact that the covoriance of $\\bar{X}$ and the random vector ${X_i-\\bar{X}}$ is zero, they are indenpdent, so is the functino of independent variables.\n",
    "3. Refactor sample variance using $\\sum{(\\frac{(X_i-\\mu) + (\\mu-\\bar{X})}{\\sigma})^2}$ and use independence of $S^2, \\bar{X}$ and mgf to derive that the final mgf is the mgf of $\\chi^2(n-1)$\n",
    "4. Is a result of 1-3 and the definition of student distribution.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
